# Snow Rendering, Deformation, and Simulation

## Overview

### Goals

We want to perform real-time rendering of snow, handling some level of interactivity in the form of deformation, accumulation, and simulation.

In particular, we're aiming for some level of calculation of snow *displacement*, so we can plausibly approximate movement through or interaction with non-trivial snow height.

We can compare this to [height-map based approaches](https://www.gdcvault.com/play/1020177/Deformable-Snow-Rendering-in-Batman) that model snow *flattening*, which works well for ankle-height snow. We can regard this type of method as modelling footsteps (or the presence of any ankle-height object) as flatenning and *compressing* the snow, alleviating the need to consider the movement of that snow.

We expect that this simulation will require significant computational power, perhaps best supported by [compute shaders](https://www.khronos.org/opengl/wiki/Compute_Shader), so we'll focus on high-end devices and pipelines supporting this.

### Aspects of Snow

We need to consider the following three categories in order to render real-time interactive snow:

A. Shading. In particular, can we come up with a convincing [physically-based rendering (PBR)](https://en.wikipedia.org/wiki/Physically_based_rendering) surface model?
B. Placement. Where do we evaluate the shading of the snow's surface? This depends on how we model the snow position, but likely it will be a mesh. That mesh may be updated in real-time to contain displacement details; with techniques such as [parallax occlusion mapping](https://en.wikipedia.org/wiki/Parallax_occlusion_mapping) or [tesselation shaders](https://en.wikipedia.org/wiki/Tessellation_shader) only the displacement would need to be updated. If we choose to do a particle-based simulation, building the surface mesh *may* not be the best approach. We may be better off taking an approach similar to something like [screen-space fluid rendering](http://developer.download.nvidia.com/presentations/2010/gdc/Direct3D_Effects.pdf), used for real-time rendering of particle-based fluid simulations.
C. Simulation. How do we simulate snow displacement and deformation interactively based on objects moving through it? As needed, how we do update the mesh representation? How is the snow modelled (2D density field, particles, etc.)?

## Project Stages

We'll approach each of our challenges (shading, placement, and simulation) in stages of increasing complexity.

### Stage 1: PBR Snow on Flat Surface with Height-Based Deformation

We'll use Unity's [High-Definition Render Pipeline (HDRP)](https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@7.1/manual/index.html), since we're already targeting high-end platforms that are Compute Shader compatible.

Materials in HDRP offer many different PBR [surface types](https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@7.1/manual/Material-Type.html), supporting translucency and subsurface scattering, which we expect may be needed to properly model lighting of snow. Additionally, these materials support pixel or tesselation-based [displacement](https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@7.1/manual/Displacement-Mode.html).

By beginning with a flat plane of snow whose displacement is modelled by a height map, we may be able to achieve (A) shading and (B) placement simply through proper material setup. We consider using a [layered shader](https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@7.1/manual/Layered-Lit-Shader.html) to handle cases of rendering the surface underneath the snow, when needed.

To handle (C) simulation, we can reference the excellent presentation [Deformable Snow Rendering in Batman: Arkham Origins](https://www.gdcvault.com/play/1020177/Deformable-Snow-Rendering-in-Batman). We'll also target ankle-height snow, where objects interact with the snow by flattening it. The update steps each frame are roughly as follows:

* Within the volume of the height of the snow, render the height of all objects found in that slab.
* Blur that height 'splat map' to *roughly* model the fall-off in height of compacted snow, and take the minimum with the height of the snow's displacement height map.
* Update the normal map appropriately using a technique like [reoriented normal mapping](https://blog.selfshadow.com/publications/blending-in-detail/).

### Stage 2: Height-Based Deformation on Top of Height-Field Based Surface

Instead of a flat surface, we'll consider snow on top of a surface defined by some sort of height field, as terrain systems [often are](https://docs.unity3d.com/Manual/terrain-Heightmaps.html).

If we expect that the curvature of this surface is not too large (compared to the maximum snow height, which is still around ankle-high), we can still treat displacement as purely vertical.

While we must update the normals appropriately, the main challenge will come in rendering the height splat map to account for the changing height of the base of the snow.

### Stage 3: Simple Displacement

Height-based deformation will only ever compact/remove snow. We must account for objects' motion in order to displace snow. Using our height-based setup, we can accomplish this by transferring snow height from one horizontal position to another based on the frame-by-frame object movement.

Similar to rendering the height of objects within the snow region, we can render object motion vectors to get a pixel-by-pixel map of horizontal movement from the previous frame. Using this, we can approximate where snow should be moved from and to on a frame-by-frame basis.

### Stage 4: Complex Displacement Simulation

Instead of treating snow as a surface, we could treat is as a collection of particles, in the same manner of [state-of-the-art](https://www.disneyanimation.com/publications/a-material-point-method-for-snow-simulation/) snow rendering simulation. 

Rendering these particles could be done using screen-space methods, inspired by techniques such as [Screen Space Fluid Rendering](https://dl.acm.org/doi/10.1145/1507149.1507164).

While particle-based approaches can be computationally expensive, we could take advantage of [recent advances](https://dl.acm.org/doi/abs/10.1145/3386569.3392431) in snow simulation to potentially achieve real-time execution alongside convincing simulation results.

## Project Setup

We begin this project using Unity 2019.3.9f1 with HDRP 7.3.1, creating the project using the High Definition Render Pipeline template.  We removed the ExampleAssets folder, and their associated contents inside the sample Scene.

We will later update to use Unity 2020.3.6f1 with HDRP 10.4.0.

## Shading

We'll begin shading with the assumption that snow's surface will be defined by a mesh. This will be a useful exercise even if we end up with a different surface representation by the end of the project.

### Basic Material Setup

As a starting point, we'll use an existing [PBR Texture Set](https://cc0textures.com/view?id=Snow006) to see how far we can get with an HDRP LitTesselation Material. We'll note that in this case, the normal map is [Direct-X style](https://docs.cc0textures.com/directx-vs-opengl-style-normal-maps.html), whereas Unity uses OpenGL style. We'll need to invert the G-channel accordingly (this can be done in GIMP by going to the Channels tab, selecting only the Green channel, then choosing Colors > Invert). We'll also ensure that import settings are set appropriately; namely, that the normal is Texture Type 'Normal Map' and the non-color textures (ambient occlusion, roughness, displacement) have sRGB (Color Texture) unchecked.

To pack the maps into the texture channels expected by the Material, we'll use [SmartTexture](https://github.com/phi-lira/SmartTexture), which can be installed as a Package. The advantage is that it will author Texture Assets with dependency tracking. It also allows us to invert the Roughness texture, to pack it as Smothness in the alpha channel. Note that we'll need a pure-white Texture of the same size to supply the missing constant maps, Metal and Detail Mask. Since snow is Dielectric and we have no Detail Mask yet, we'll invert it so both of those channels are black.

We'll come back to the displacement map later, noting that the physical size of the texture is important in determining any vertical offset, as well as the strength of the applied normal map.

Let's start by visualizing the Material on a flat Quad, which is 1m x 1m square. Since this texture contains footprint imprints, we can use those to estimate the appropriate size. A footprint is 0.167 Units long, unscaled, which we approximate as roughly 10.5 inches, or 0.2667 m. So, we should scale the texture by 0.167 / 0.2667 = 0.626 to match the Unity convention of 1 m per Unit, or scale the Quad to 1.6 m. We'll deal with height/displacement scaling later.

### Subsurface Scattering

This is a fine starting point, but despite capturing snow's high albedo, the result looks a little more like a hard concrete or plaster surface. Since snow is made up of [ice crystals](https://science.howstuffworks.com/nature/climate-weather/atmospheric/question524.htm) and is therefore translucent, we need to go beyond a simple surface model. Let's change the Material type to Subsurface Scattering, turn Transmission on, create a new Diffusion profile under Create > Rendering > Diffusion Profile, and assign it to our Material.

The Diffusion profile should be determined by physical measurements, but for now we'll just focus on getting the look in the right ballpark. We can keep an index of refraction of 1.4, about that of water. We're working with the default World Scale of 1 Unit = 1 m, so we'll leave that value at 1. We'll significantly boost the Scattering Distance, keeping an HDR white color with Value 100, Intensity 1. This will bring the blue radius of the subsurface scattering to over 20 mm.

While we don't have a quantitative reason to choose the value of scattering distance, the visual results are reasonable. We're just eyeballing our results for now; we could potentially revisit these values later if we wish to do reference photograph comparison. Already, we see an improvement, softening the lighting.

### Displacement

We notice that the introduction of the subsurface scattering has washed out much of the effect of the normal map, making the result look fairly flat. Since we have a displacement map, we know there is at least some height variance to the surface. Let's use that displacement map by choosing [Displacement Mode](https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@7.3/manual/Displacement-Mode.html) > Tesselation Displacement, and assign the displacement map to the Height Map field. Let's also turn off both Lock Scale options, so we'll set the height scale directly.

The Tesselation Factor will determine the strength of the tesselation effect, and should really be chosen in conjunction with the resolution of the base mesh. We'll replace the Quad with a Plane; since it is default size 10x10, we'll scale it to 0.16 to match the size of the Quad, but give the base mesh a reasonable amount of tesselation to start with. A Tesselation Factor of 21 will now giveus very good detail -- we can worry later about performance and appropriate choices of tesselation factors in different scenarios. We'll choose Phong tesselation mode for now, and reconsider later if the displacement looks too smoothed as a result.

What should the height range be? We'll keep the MinMax parametrization, with Min of 0 cm (undisplaced). We need some sort of estimate of the actual height range of the material. This is a little tricker than trying to decide the horizontal size, where we had a convenient footprint for scale. It looks like fairly flat, packed snow, so we'll go with a Max value of 8 cm, and tweak until it looks right.

Thanks to the Tesselation, we see that the raised portions will cast shadows. However, the shading doesn't quite look quite right. To see this more clearly, let's go to the scene's Directional Light, and turn off both the Shadow Map as well as Contact Shadows, leaving us with just the lighting. As a test, let's increase the Max height up to 50 cm and beyond -- note that the lighting doesn't change. This means that the displacement doesn't change the surface normals. They are affected only by the normal map, which we can test by reducing Normal Strength to 0 and noting that the lighting response behaves as though the surface was a flat plane. Using Window > Render Pipeline > Render Pipeline Debug, we can easily visualize the normals. Particularly, in Material > Lit/Geometric Normal, we see a constant value.
All this means is that we must choose the Tesselation height to be consistent with the given normals from the Normal Map.  Since we can only estimate the displacement height, this means that we may need to change the Normal Strength multiplier so that the resulting surface normals match what we would expect for each Tessellated triangle (at least on average over the triangle -- there may be additional detail in the normal map).

We'll eyeball an increased Normal Strength. In the Render Pipeline Debug Window, we'll choose Common Material Property > Normal. We'll note that the map is mostly green (normals pointing up in world space), but for the tall bumps, the sides are more red or blue, indicating the the normals are pointing more to one side or the other. We'll note that it looks like only a thin strip on each side is sampled from the normal map, and it doesn't cover the whole side of the surface of the bumps. Increasing the Normal map strength to 3 or 4 will give better agreement between what would be the true geometric surface normal (from Tessellation, not used in shading) and the values sampled from the Normal map.
Turning off the debug, we see the height much better defined by the normal map, and less washed out. Leaving aside the correctness for now, we have the start of a good-looking snow surface.

### Transmission

The Diffusion profile also supports handling transmission of light through the back of the material to the front. Since we have displacement, our snow is no longer flat, so let's look at transmisison of light through raised areas of the snow. We note that we don't have an [average thickness map](https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@6.7/manual/Material-Type.html) to assign to the material, and no straightforward way of assigning it. In most cases, snow will be lying on an exterior surface, so transmission will occur primarily in cases where the sun is shining horizontally through a raised portion of snow. Therefore, for Transmission Mode, we'll choose Thick Object type. The [difference](https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@7.1/manual/Diffusion-Profile.html#working-with-different-transmission-modes) is that Thick type derives thickness from the shadow map, not just the thickness map. The [Thickness](https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@6.7/manual/Material-Type.html) Material slider sets the strength of the transmission effect. For Thick object type, setting this to zero seems to result in incorrectly high transmission for Thick objects (using the default Sphere to test, for example), so we'll leave it at one.

Heading back to the Diffusion profile, we'll keep the Texturing Mode to Pre-and Post-Scatter, for the softer, natural look. We'll remap thickness between around 0 and 16 mm. As with the scattering distance, we're just choosing thie value by eye for now. We'll leave the Transmission Tint as white.

### Smoothness and Iciness

By changing the Material's Smoothness Remapping Range, we can make the surface more or less smooth overall. By increasing the minimum value of this remapping range, we'll make the rough parts of the surface smoother, and the resulting specular lighting will make the snow surface look more like ice or frost. A remapped minimum around 0.5 looks good. Set it too high, and the reflections will shrink to noisy, speckly pixels resulting from high frequency changes in the normal map. This looks somewhat like what we might expect from a 'sparkly' snow effect, but is too noisy and makes the lighting response look diffuse everywhere else.

In contrast, lowering the maximum value of the remapping range will make the snow look softer and more diffuse.

There are many, many [types of snow](https://en.wikipedia.org/wiki/Classifications_of_snow) and we won't try to render the full range, or even any one type in particular. Like the smoothness, we'll try to note the parameters we have reasonable artistic control over.

### Understanding Sparkle

Snow is composed of ice crystals, each with an individual orientation. If the orientation is right, light reflection off of an individual crystal can give snow a [sparkle effect](https://uwmadscience.news.wisc.edu/atmospheric-science/just-look-at-that-snow-sparkle/).

Given that a snow crystal's diameter is [around 0.5-1.0 mm for a medium sized crystal](https://unesdoc.unesco.org/ark:/48223/pf0000186462), we won't be trying to capture their orientation in any meaningful way with a normal map. This noise is [too high frequency](https://en.wikipedia.org/wiki/Nyquist_frequency) even for a map texture with 4K pixels covering 1m to represent.

#### Detail Map Approach

We note that the HDRP Lit material offers a Detail map, where we pack channels with Albedo, Normal Y, Smoothness, and Normal X into the RGBA channels. Each channel represents deviation from neutral at a grey value of 0.5. Could some combination of detail Normal and Smoothness create a convincing effect?

It's fairly unlikely, since the PBR lighting model we're working with treats the roughness of a surface as described by the orientation of many microscopic scale microfacets, where the smoothness describes the uniformity of the [microfacet orientation on average](https://learnopengl.com/PBR/Theory).

Regardless, let's see what we can achieve, and what we can learn. The Render Pipeline Debug window's Lighting tab will be helpful, particularly Lighting Debug Mode, where we can look at diffuse and specular lighting contributions separately.

What are we looking for in this detail map? To model this random-orientation specular lighting, we'd like a high specular response at a sparse set of pixels, where that response also depends on the camera direction. The detail map only modifies the existing smoothness or normal values, so the detail map must also be sparse -- it shouldn't affect the lighting except at the sparkle points.
I'll use Substance Designer to create these maps, but any image editing tool would do. We'll start with white noise and remap the levels so that only a few white pixels remain, and the rest are mapped to black. To get smoothness, we'll remap black to grey, representing neutral (no change to smoothness), keeping the sparse white pixels (maximum smoothness). We'll also create a normal map from another white noise input (using a different random seed) and blend it with the sparse mostly-black map. This will get a neutral normal map except at the same set of pixels that have maximum smoothness, where normal values will be random. We'll leave albedo as a neutral gray, and pack these maps into the appropriate output channels.
We'll export the map to an image and ensure sRGB is unchecked and compression is set to None, to preserve detail.

Let's try it out, by setting the Material's Detail Map, increasing the UV tiling to around 3, and setting the detail scale for normal and smoothess to the maximum value, 2. We very clearly so no sparkle, or really much change of any kind. Let's investigate by setting the Render Pipeline Debug window Lighting Debug mode to SpecularLighting. As we rotate and move the camera, we can already see a lot of speckle-like noise. This isn't coming from our detail map, which we can see by setting the detail scale strengths to 0.
Clearly there's some form of anti-aliasing for the lighting going on under the hood in HDRP. If you look closely, you can see the same happens for the DiffuseLighting, though it's less noticeable. As we move the camera away from the surface, this effect is less apparent. The same is true if we get really close to the surface, so that one texel in our material map takes up many pixels on the screen. Clearly, there is a need to deal with aliasing issues that techniques like [mipmapping](https://en.wikipedia.org/wiki/Mipmap) are meant to overcome.
Let's look at the Material Smoothness in the Render Pipeline Debug window, setting the detail scale values back to maximum. Most of the detail smoothness can be seen only when the camera is extremely close to the surface; otherwise, it is averaged away by the changing mipmap level. To keep the high-frequency aspect of our noise, let's try disabling 'Generate Mip Maps' in the texture import settings. Note that we may wish to instead bias the mipmap level so that at very far distances from the camera, we don't suffer from extreme aliasing; however, this is not possible directly in the Material editor. We can now clearly see speckles of maximum smoothness; furthermore, due to the aliasing, moving the camera will cause this speckle pattern to shift. We can see the same for the normal values, though the effect is less pronounced.
Next, let's debug the Specular lighting. You can just see an additional bit of a speckle pattern above the other noise that we mentioned earlier. By setting smoothness remapping to zero, it's easier to see how the detail map adds speckles to the specular lighting. However, _even in this case_, we can't really distinguish an effect on the final material. Our aliased addition to the smoothess simply doesn't create a strong enough specular response.

While this was an interesting attempt, we'll remove this detail map, which doesn't achieve any appreciable change to the look of the snow.

#### Emission Approach

We'd like to see if we can at least approximate the type of sparse, bright lighting we might expect from a sparkle effect. We'll use the Emission property, creating a similar mask to the Detail Map approach we tried, mostly black, with a sparse scattering of white pixels. We'll assign it to the Emmissive Color texture in our Material properties, setting the tiling to 3. We'll use Emission Intensity instead of choosing an HDR color, so we'll set the Emission color just to white.

Next, we'll need to choose the intensity. We'll want it to be bright enough that it can still be seen above the normal lighting response of our snow material, which will depend on the scene's light intensity. For our sample scene where the directional light has intensity 5000 Lux, an emission intensity of around 17 EV100 is about right. The [exposure weight](https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@7.1/manual/Lit-Shader.html#emission-inputs) slider affects how much the camera [exposure](https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@7.2/manual/Override-Exposure.html) has on the emission power. Lowering this slightly from 1 to 0.98 or so can help make the light show up a little better against the lit areas of the snow's surface.

The results look reasonable, at least in terms of the distribution, spread, and intensity of the points of emissive light. Increasing the intensity slider, however, we see that there is a very careful balancing act to keep the intensity both noticeable and believable. We would need to choose the intensity to match that of the lighting.

There are other significant, obvious issues with this approach. Since lighting is not taken into account, we see emissive light even on side of the snow surface that are not directly lit! Of course, the intensity should depend on the lighting response at a given point on the surface. Also, since emission is mapped by a texture, the emissive lighting doesn't change with respect to the camera. While we could try to randomly change the tiling offset with camera movemement, or interpolate between different emission maps with camera movement, but given the general incorrectness of this approach, it's not worth it to try.

#### Custom Lighting

In both approach attempts (detail map and emission) we came across difficulties because they did not fundamentally alter the specular lighting response. One altered the parameters only, while the other added independent lighting. Clearly, we'll need to write a shader with custom lighting to properly handle this.

We note that ShaderGraph allows us to procedurally customize material properties easily, but does not expose a simple way to adjust the lighting. Furthermore, it doesn't currently support tesselation and the associated displacement. This means we'll need to write our own, customized version of the HDRP LitTessellation shader. There are great resources describing Unity's Default Rendering Pipline, like [this set from Catlike Coding](https://catlikecoding.com/unity/tutorials/rendering/), which helps navigate the structure of Unity's shader code. Without equivalent resources describing HDRP, the bet to understand what's going on may just be to look at [the source](https://github.com/Unity-Technologies/Graphics). Just setting up all the different passes, keywords, includes, and material properties for the Lit shader takes over a thousand lines! To try to create a custom version is an enormous amount of work just to begin working on a visual feature that's nice-to-have.

We'll come back to custom lighting once we've made progress on deformation and simulation.

### Layers

#### Height Blending and Setup

As we model snow height, we must also pay attention to the surface upon which the snow sits. Furthermore, the implementation we'll be re-creating in our first stage of height-based deformation is to consider two snow materials: one for the compacted snow, and one for the undisturbed snow. Both height and material properties are changed in response to footsteps or other objects.

Thankfully, HDRP offsets a [layered shader](https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@7.1/manual/Layered-Lit-Shader.html) that allows us to combine up to four materials based on an influence texture, and optionally based on the height of the materials at each point. Using this, we may be able to combine the compacted snow, undisturbed snow, and underlying ground/surface materials into one single Unity Material.

Let's see how far this out-of-the-box solution can take us. We'll follow the same [material setup](#basic-material-setup) with another set of [snow PBR texture](https://cc0textures.com/view?id=Snow005), keeping the height range to 2 cm (and normal map strength of 1). We'll also reduce the maximum of the smoothness remapping range, so it doesn't look quite so icy. We'll setup another material based on [ground textures](https://cc0textures.com/view?id=Ground021), with a height range of 4 cm with a normal map strength of 2. Before changing Surface Type from Subsurface Scattering to Standard, we'll create and assign an 'empty' diffusion profile with zero scattering distance, and set the subsurface mask to 0. This will help later when we assign our layered material.

We'll make a new HDRP LayeredLitTessellation Material, choosing Surface and Tessellation Options as before. Under Surface Inputs, we'll set the layer count to 3. We'll come back to the layer/blend mask texture and tiling later. Let's go to the Layer List and set the ground material as the Main Layer, which will the base underneath the other layers. We'll set our second snow material as layer 1, which we'll consider to be just on top of the base, and set our original snow material to layer 2. We see that the material details for each layer have been copied, and can now be changed independently from their original materials.

Without a Layer Mask set, we just see the topmost layer, Layer 2. Let's explore height-based blending, enabling that option in the Surface Inputs section, keeping the Height Transition scale as 0. Let's lower the Height Map Offset for Layer 2, to -4, so that layer will shift downwards and expose the second snow layer below it, with a roughly equal mix. We'll also start to see some of the higher stones of the main ground layer poking through, though the blend looks a little artificial in this case, at least up close. Let's therefore reduce the main layer's max height to around 1 cm, so no stones will stick up through the bottom later of snow, and its normal map strength to 1, to match. We don't expect the height of layers to be independent, since snow should be falling on top of the ground, even at the highest spots.

Additionally, let's enable Main Layer Influence in the Surface Inputs settings. Now we can set the main layer to influence base color, normal and height for the other two layers. It won't make much sense for the top layer of snow that's piled on top of the ground, but it should have some influence over Layer 1, which is a relatively thin layer sitting just on top of the ground. We'll set base color influence to 0.15 and normal and height influences to 0.5. We see a little bit of the structure of the ground underneath on the normals of the lower snow layer.

Now that we've done this, let's set the Layer 2 Height Offset to -3. Note that the footprints are now more readily apparent, as they seem to expose the second snow layer below. This is a good look to try to target when the amount of fresh snow cover is not too high.

#### Layer Mask

Let's now add a Layer Mask texture, which controls the strength of layers 1-3 in RGB channels, and the main layer in the A channel. Note that higher-numbered layers will show on top of other layers, unless the 'Use Height Based Blend' option is enabled. For now, we'll make a copy of our previous material, turning off height blending. We'll focus on showing mostly the top layer (Layer 2), with some spots of the layer below (Layer 1) showing. For now, we won't see the Main Layer directly, just its influlence that we've added to Layer 1. Since both the main layer and layer 1 will always be found underneath our main layer, their corresponding channels (R and A) in the layer mask will be white everywhere. The blending will come in the G channel, controlling the strength of layer 2.

Let's create a layer mask that features a mostly full-strength top layer, except for elliptical regions where the strength will go down to zero, roughly simulating footsteps. I'll use Substance Designer to create this map, but any image editing tool would do. The G channel will be mostly white, with a few scattered black ellipses. We'll blur them so that the transition between layers is not abrupt and artificial. As mentioned before, R and A channels will be white. Without a third layer, we'll leave the B channel black.

We'll assign the Layer Mask and ensure that all height offsets are zero for all three materials. The result looks reasonable, though balancing the transition between layers is difficult. We'd like to see a well-defined transition, but we are limited by the resolution of our material maps, as well as the tessellation being used. The results look reasonable, though there's not a significant height difference between the masked and unmasked areas showing Layers 1 and 2.

We can increase the offset of Layer 2 above zero to really see an obvious footprint-type impression. Even a 2cm offset is enough, since the entirety of the top snow layer is above the highest point of the snow layer below. However, note that the Normal values (which we can check in the Render Debug window) do not change as a result of the height changing. We already knew this was the case for the single-layer tessellation material. However, in this case, we are blending between height maps with different offsets, which means that the blended normals will be significantly incorrect. To see this, simply set the Layer 2 offset to a large value; at 50cm and -50cm it is clear that the lighting does not respond as we'd expect, since the blended normal map doesn't take into account the height map ranges. (Again we can check the Render Debug window and see that these large offsets are simply stretching the normal map).

What if we turn on height blending? Starting at Height Transition size of 0, we see a clearly incorrect instantaneous height transition, which looks like it happens at the point where the layer 2 mask channel goes below some cutoff value -- perhaps something like a half. By increasing the transition size, the transition is softened; at the maximum value of 1, it looks similar to the mask-only blending. With this setup, height blending therefore isn't needed. In this case, since the two snow layers cover completely different height ranges (the top layer's minimum height is above the bottom layer's maximum height), we don't need to take advantage of height based blending. With this setup, we take full responsibility for the blending; in this case, for the top snow layer's mask texture.

If we look closely, we'll see another issue with this heightmap-based deformation, when there's a relatively fast change in height, as is the case here blending these two snow layers. The material maps are sampled based on the underlying uv values of the mesh. Since we started with a plane, these are just a stretched and offset version of horizontal world coordinates. As a result, we can see the texture is stretched vertically in the transition region. Due to tessellation, we have triangle coverage, but they are more vertical than horizontal due to the rapid blending between layers. However, sampling of all textures (albedo, normal, etc) happens only based on the (horizontal) uv coordinates.

In principle, this could be alleviated with a different type of texture mapping, such as [triplanar mapping](https://catlikecoding.com/unity/tutorials/advanced-rendering/triplanar-mapping/). As this would involve altering the built-in shader, just like the custom lighting for the sparkle effect, we'll save this for later, if it ends up necessary, depending on our final choice of underlying mesh for the snow placement.

Nonetheless, despite some of the small issues, this approach can give some reasonable-looking results, particularly if we choose moderate height displacement ranges and offsets. To update this effect in real-time would simply mean updating the layer mask texture. Since only the G channel is used, the corresponding blend texture behaves similarly to an overall height map, if the layers as sufficiently differentiated in height.

#### Masking Height

In the previous setup, the layer mask consisted only of one non-constant channel, the one determining the strength of Layer 2, the top layer of snow. As this decreased from 1, the material was increasingly blended to the Layer 1 below it, changing the surface properties, and correspondingly decreasing the height. As discussed, this mask map is very similar to a height map. We may therefore ask the question: can we combine the top snow layer's existing height map with this mask texture, bypassing the Layer Mask blending and relying only on the Material's Height Blend option?

Even if possible, we'll run into similar issues as before, where we are altering the height of the top layer, but not reflecting these changes in the normal map. As discussed, with this Tessellation displacement setup, the per-pixel normal values are affected only by the normal map. At least with a static mask height map, we can potentially calculate the corresponding change to the normal map.

I'll use Substance Designer for this. Combining height maps is a relatively straightforward remapping and multiplication blend operation. If we're using height blending, then the final heightmap texture for the top layer should represent the entire height range from the bottom of the bottom layer to the top of the top layer. This way, the top layer can be fully represented if there's no masking, but can always be shifted below the bottom layer when fully masked, so the bottom layer will show due to the height blending. As a result, this will depend on both layers' height ranges and height offset values.

Thus, we'll finalize the height ranges before we work on the combined height map. Layer 1 has a range from 0 - 2 cm, with no offset. Layer 2 has a range from 0 to 8 cm, with a 2 cm offset, for a total of 2 - 10 cm. Thus, our overall range is from 0 - 10 cm, where black in the height map will map to 0, and white will map to 10 cm. In general, if L is the minimum height (including offset) of the lower layer, U the maximum height of the upper layer, D the maximum height of the lower layer, the M the minimum height of the upper layer, then we want to remap the top layer's height map encoding values mapping to [M, U], to those mapping [L, U], by the following expression, where x is any given heightmap value ranging from 0 to 1: x + (1-x)(M-L)/(U-L). Values in the remapped texture will be in the range [M/H, 1]. With L = 0, M = 2 and U = 10 cm, then (M-L)/(H-L) = 2/10 = 0.2. Essentially, we're remapping the previous heightmap so values that used to be 0 (black) are now 0.2 (dark grey), while values that used to be 1 (white) stay the same.

Setting this as Layer 2's displacement map, removing the layer mask, enabling height blending, and remembering to set Layer 2's height range to the full range from 0 cm to 10 cm, we see somewhat similar results to the layer mask case. However, even with the height transition distance set to zero, the pseudo-footprints are overall shallower. The transition between the material surfaces is sharp, since there is no transition distance. Increasing the transition distance only shrinks the pseudo-footprint size, soon leaving the bottom showing only the top layer's material, never seeing the bottom layer.

Compared directly to the layer mask approach, it seems that this method is a little less flexible, since we cannot increase the height transition size without having to adjust the layer height ranges or offsets to compensate, thus necessitating a change to the combined height map, leading to a circular dependency.

#### Normal Blending

If we continue along the heightmap blending line of thought, we can potentially consider calculating a combined _normal map_ in a similar manner. We don't yet have a normal map corresponding to the mask, and we can't simply blend as we did for the height map. In general, normal map blending is a [delicate process](https://blog.selfshadow.com/publications/blending-in-detail/). Thankfully, Substance Designer has nodes that will let us accomplish these two tasks: create a normal map from a height map (in our case, the blend mask or combined height map), and combine two normal maps. Other tools like [AwesomeBump](https://github.com/kmkolasinski/AwesomeBump/blob/Release/README.md) can achieve similar results.

Let's start by feeding in our combined height map into such a normal generation node. The only parameter is Intensity, and comparing the non-masked regions of this resulting normal with the original top-layer normal, we see that an Intensity of 1 is clearly too low. Of course, given a physical size of the texture and a corresponding physical size of the height map range, the (unit) normal vector is given by [partial derivatives of the heightmap](https://en.wikipedia.org/wiki/Normal_(geometry)). Of course, there are sampling issues to consider as well, since we're dealing with a finite-size heightmap texture. We'll just look at the difference between our generated normal and the original normal and try to choose a strength to make the more prominent features look similar. We'll note that these normal maps are meant for slightly different height ranges; the original for a height range of 8 cm, but the combined one covers a range of 10 cm. We won't worry about these details for now; we can do a close comparison against reference height maps and normals (where we can calculate the normal map analytically) later if needed. We note that we'd need to perform these operations at runtime in a shader to account for real-time changes to the blendmap.

An Intensity of around 3.5 looks reasonably close to the previous normal map. The effect of the blend mask height on the normal map is immediately apparent. We also see that this technique may have caused some loss of detail in the normal map, since we are reconstructing it from the heightmap. Assigning this to Layer 2's normal map, we immediately see more correct shading caused by the height slope from the layer blend/mask map.

We'll also try an alternative approach with the Normal Combine node, choosing the 'Detail Oriented (High Quality)' option. As the base input, we select the existing top layer normal map. We'll take the blend mask as a heightmap and use the Normal node to convert it to the second normal map to be combined. As before, this needs an Intensity value, and by comparing the difference with our previous combined normal map, we see that an Intensity of around 2.7 achieves a similar falloff for the normal map due to the blend mask. Note that this normal combination will result in the fully-masked areas still containing normal detail from the top snow layer, despite the fact that the heightmap is perfectly flat here. We'll acknowledge this deficiency, but since the lower snow layer will be showing at these points, this is an acceptable inaccuracy. The difference between these two normal-generation approaches is not too big, but we'll go with the second one, since it lets us keep the previous normal map detail intact.

By combining the blend mask as a heightmap to affect the top layer's displacement and normal maps, we can get tessellated height with appropriate normal vectors for the now non-flat geometry. We couldn't get that with simple heightmap and normal blending between the two layers, due to their different height ranges.

### Future Shading Work

* Sparkle effect (custom lighting response).
* Planar Mapping, so any snow surface can be mapped without needing to supply appropriate uvs for the snow material.
* Normal blending: determine normal from height in shader, separately from detail normal. Normal to height reference comparison, including different height range on masked version.
* Triplanar Mapping, so rapid height changes with mostly-vertical surfaces can still have appropriately mapped detail, unlike the case where mapping is done through horizontal coordinates.
* Determine the appropriate normal strength based on the displacement height.
* Ensure response to baked lighting is appropriate.
* Reference photo comparison, to adjust subsurface scattering, custom lighting, and other material properties.
* If useful, create and apply the Subsurface Mask to modulate the strength of the subsurface scattering effect.

## Placement

We'll start work on updating the placement of snow. To begin, we'll build upon the layered materials discussed above, updating the blend mask as appropriate.

### Orthographic Capture

In this technique, we focus on capturing the height of snow on top of a flat surface, modelling snow compression depending on the height of objects above that surface. Since the surface is flat, we can capture this volume of interest (a rectangular prism) by an orthographic camera. We'll begin by adding a Unity Layer for 'GroundOrSnow' to ensure we only capture other objects above the surface, assigning all of our ground and snow surfaces to that layer.

To begin with, we'll do as much as possible with a manual setup to understand the necessary elements. We'll late automate these with scripting, perhaps by integrating with the HDRP pipeline through a [custom pass](https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@7.1/manual/Custom-Pass.html). Since our surface objects have size 1.6 m, we'll want to create an Orthographic camera at the same coordinates, pointing upwards (Euler x = -90 degrees), with a Size of 0.5 * (1.6 m) = 0.8. We'll use near and far clip planes of 0 and 0.1 m, to encompass our 0 - 10 cm height range. We'll also create a RenderTexture Asset in the project explorer of square size (2K, like the other maps), and assign it to the Target Texture of this new Camera. We can see that the orthographic bounds exactly encompass the volume we'd like them to.

Next, we'd like to ensure that the camera does only a depth pass. It doesn't look like there's an easy way to do so in HDRP. We'll turn the Camera component off, so it won't be part of the rendering loop. We'll need to add a custom pass that will bake the depth from that camera. Thankfully, we can take advantage of this [GitHub repository of example Custom Passes for HDRP](https://github.com/alelievr/HDRP-Custom-Passes), one of which does exactly what we want: renders a camera's depth buffer to a RenderTexture. We'll update our RenderTexture to also use a Color Format of DEPTH_AUTO. This will keep that RenderTexture updated with the main camera's render loop. We'll create a new GameOject to hold the Custom Pass Volume, with Mode set to Global and Injection set to Before Transparent. It's unclear why it should need to be at this point and not earlier (since we're using an entirely different camera), but if we choose to inject an an earlier point, the entire render loop will break, at least in Editor; we'll come back to this later. We'll assign the target camera and the RenderTexture we've created, leaving Target Color and Depth buffers as Camera, with Clear Flags as None. We just need to update the layer blend mask from that texture.

#### Updating Layer Mask

For our first implementation, we'll update the layer blend mask to always _lower_ the height of the snow. It seems that [Custom Render Textures](https://docs.unity3d.com/Manual/class-CustomRenderTexture.html) are designed precisely for the use case of updating a texture with a shader. We'll create one with the same size map as the statis layer mask map, using standard R8G8B8A8_UNORM format, keeping wrap mode as Clamp. We'll need to enable Mip Maps and turn on their automatic generation, which will prevent aliasing when looking at the snow from very far away. For Initialization Mode, we'll use OnLoad type, since we don't need it to happen every frame. Source can be Texture and Color, with no texture and white color, indicating that we're starting with all layers present. Change the UpdateMode to Realtime with Period 0, so this Custom Render Texture will automatically be updated every frame with the given material, which is the last thing we must create.

By starting with an [example of a Custom Render Texture shader](https://docs.unity3d.com/Manual/class-CustomRenderTexture.html#writing-a-shader-for-a-custom-render-texture), we'll assign as a Texture the RenderTexture written to by our depth Custom Pass. Since we've set up our surface as a square covering UV range 0-1, and the orthographic camera covers exactly that area, we the UV coordinates line up, except that the x-coordinate must be mirrored, since the camera renders from below and we are visualizing from the top. In the Pass settings, we want to fully blend all channels, so we will use `Blend One One`, but we'll want to take the minimum of the existing and incoming values, using `BlendOp Min` [blend operation](https://docs.unity3d.com/ScriptReference/Rendering.BlendOp.html). We'll sample the passed-in RenderTexture using SAMPLE_DEPTH_TEXTURE. Since orthographic cameras render depth linearly, we only have to worry about the depth range being reversed, as denoted by [UNITY_REVERSED_Z](https://docs.unity3d.com/Manual/SL-PlatformDifferences.html); see the shader code for details. We'll make a Material with this Shader, and assign it to our Custom Render Texture.

Assigning our Custom Render Texture to the Layer Mask of our surface Material, we can see that the blending is indeed being updated correctly, and being retained over time. Note that the initialization of our custom render texture only happens when it's loaded, not on scene start. So, we'll change it to On Demand, and write a simple Component script to update it on Start, with the `[ExecuteInEditMode]` attribute so the texture is re-initialized both entering and exiting Play mode. We'll add that Component to the same GameObject holding our Custom Pass Volume.

With this, we're able to update the Layer Mask to show snow compacting. The most immediate issue is that while the displacement used in tessellation is blended, the normals are not blended appropriately, due to the height displacement ranges being different among the two snow layers. We'll return to this in the next section.

Another limitation of this straightforward masking approach becomes more apparent when we're dealing with exact height values captured from objects in the scene. Our mask represents a height fraction from zero to 1, covering the range 0 to 10 cm of our layered snow material. However, we note that the top layer only reaches a _maximum_ height of 10 cm, but on average will be somewhere around the middle of its range, around 6 cm. Therefore, where an object is at 9 cm above the ground (undisplaced position), our depth capture will record a value of 0.9 for the layer mask. However, it's likely that at that point, the top snow surface doesn't reach a height of 9 cm. At those points, we expect intuitively that the snow should not be deformed, and yet we are already beginning to see some deformation towards the lower layer. The same is true for the lower snow layer; once object reach that layer's current height, we would expect to see the top layer fully masked out. We don't, because an object needs to reach 0 cm (almost certainly below the bottom snow layer) for a full masking. Nonetheless, when the camera isn't too close, this isn't really a big deal, since any objects on the ground will probably obscure the view of these snow areas, and once the object has moved out of the way, users likely will not notice this discrepancy.

One other limitation is that edges of objects can create very sharp transitions in the layer mask. Therefore, we'd like to blur the captured depth texture before applying it to the layer mask. To do this, we'll update our depth bake custom pass significantly. We'll again look for inspiration in the [GitHub repository of example Custom Passes for HDRP](https://github.com/alelievr/HDRP-Custom-Passes), this time looking at the Blur example. We'll need to make several changes, since it uses the `RTHandle` [system](https://docs.unity3d.com/Packages/com.unity.render-pipelines.core@10.2/manual/rthandle-system-using.html) to manage the use of RenderTextures. However, it seems that much of that system is meant to handle multiple cameras, changing screen sizes, stereoscopic views, and so on. We just want to write to a fixed-size RenderTexture, so any temporary RenderTextures we create for the blur passes will be correspondingly fixed. Since writing into depth-format RenderTextures seems difficult, we'll instead change the format of our final output RenderTexture to R16_UNORM with no depth buffer, which will hold the blurred depth from the orthographic camera. As a result, we'll need to create a temporary RenderTexture for our pass to hold the initial depth result as rendered by the camera, using `colorFormat: GraphicsFormat.None, depthBufferBits: DepthBits.Depth16`. We'll use that example Blur CustoPass, and add the horizontal and vertical blur passes after we render the depth. We'll need another R16_UNorm RenderTexture to hold the result of the horizontal blur pass. We can also base our Blur shader on the example `Blur.shader`, but we'll just sample all source textures directly (since size is fixed) using 'classic' Shaderlab `sampler2D` and `tex2D`. We also don't need to worry about `_RTHandleScale`, again, since we're dealing with fixed sizes for now. Finally, we'll only need to sample and write to a single channel (R).

The result from the blur, using Radius 8 (since to our textures all being fairly large), adds just enough of a smooth falloff where otherwise the transition would be instantaneous. This helps avoid some extremely artificial lighting scenarios.

We also notice some issues where the bottom of an object dips below the lowest ground level (height of 0). In this case, the front faces of the objects aren't rendered, being in front of the near clip plane. By adding a `RenderStateBlock` to the `RendererListDesc`, we can override the `RasterState` and turn `depthClip` off. Note that in the Frame Debug window, this shows up as ZClip False. We should be careful when calling the RasterState constructor to use the non-empty one, so that default values for other parameters can be provided. By doing this, we see that portions of objects below the surface bottom show up as zero depth in our captured texture.

We can test this by looking at the behaviour of a large sphere (scale of 0.5). When centered at y = 0, we see the full radius of the sphere written to the depth map at zero. Lowering the sphere doesn't change this until the sphere leaves the orthographic frustum volume, at which point the object is probably being culled from the renderers list. Moving the sphere up from y = 0, we see that the front-face depth values around the edges are written correctly, but the portion lying partly below will show up as zero. The case where some of the object clips below the ground is much more likely than where most of the object is below ground, in terms of scene setup, so we'll add this backface pass to benefit us in the former case, acknowledging that it won't work properly in the latter, which we can avoid with proper scene setup.

To avoid the issue where the mask fraction doesn't represent the final height of the blended material, we may work _backwards_ from our knowledge of the two layers' height maps. To do this, we'll add a new Pass to our shader that updates the Custom Render Texture layer mask, and add texture properties for the top and bottom snow layer height masks. We will explicitly add in the height range (as a fraction of the full range) of the bottom and top layers as a vector property to our shader. In this new pass, we will sample each of these heightmaps, rescaling the layers' values to a fractional value within the full height range. Then, we'll set our top layer mask value as the fraction between these two height values. We'll need to create a new Custom Render Texture to hold this mask result, and a corresponding Material to update it, assigning it the heightmaps and lower layer height scale. In this case, it is a range from 0 to 2 cm of the total range 0 to 10 cm, for fractional values of 0 and 0.2; for the top layer, the range is 2 to 10 cm, or 0.2 to 1. We can see that height is now properly 'carved out' exactly to the depth of the objects. A downside to this correct yet sharper height transition is that it can produce more obvious artifacts when the height transition is quick, since it happens over a shorter height (and therefore horizontal distance) range. Perhaps additional blurring of the incoming depth capture, or of the resulting layer mask, could improve this.

#### Updating height and normal maps

We'll follow the same pattern that we did with the Layer Mask with a heightmap-only version of our snow material. Here, we'll just be updating the displacement map of the top snow layer in response to the captured depth buffer. This will follow much of the same logic as with the Layer Mask case.

First, we'll extend our layer mask update shader with a second Pass. We'll make a common function to sample the linear depth value in the [0, 1] range. In this other pass, we'll return just a `float` (the depth value) in the fragment function, since we will be updating a single-channel displacement map; the Min BlendOp will remain the same. We'll double-check and make sure that our existing Custom Render Texture is using the previously-created pass. Next, we'll create a new Custom Render Texture to hold the realtime-updated displacement/height map. We'll ensure that the new single-channer shader pass is set for the material update function. We'll initialize using a _scaled_ version of the layer 2 heightmap, that covers the full height range from 0 to 10 cm. This is the same output from our height masking material, using only the remapping, without yet applying any mask.

Next, we'll create a version of our snow material based on the [height masking](#masking-height) one. We'll use our new Custom Render Texture for the displacement map of the top snow layer, and for now, use its standard normal map. Since we'll be wanting to visualize these different techniques side-by-side, we'll also create a new instance of a plane with this material, and a corresponding orthographic camera setup as described above. We'll create a new RenderTexture to hold the result of the blurred depth map of the new camera; we can worry about RenderTexture management later, depending on the particular application needs. Finally, we'll create a new Mask Update material referencing this new RenderTexture, assign it to the new Custom RenderTexture and ensure our new single-channel pass is selected. Finally, we'll add a new CustomPass to our CustomPassVolume, assigning the new orthographic camera and RenderTexture, and add a CustomRenderTextureUpdater for the new Custom Render Texture.

To sum up, in terms of new assets, we have a Plane, snow Material, Custom Render Texture for displacement, orthographic camera, RenderTexture for the camera's depth, Material for updating the CustomRenderTexture with the new RenderTexture, Custom Pass to update the RenderTexture from the camera, and CustomRenderTextureUpdater. In terms of logic, all we've done is ensure that the Custom Render Texture initialized using a version of the initial top-layer height map scaled to the full 0-10 cm range, and ensured that the new mask update shader pass does a Min blend only on the single channel of the heightmap.

The end result looks fairly good. Since we're capturing the height of objects exactly in the full range 0-10 cm, and updating the heightmap exactly, the impressions that objects leave will be as exact as the precision of our heightmap. This is a big advantage over the layer mask approach. One downside, however, is the the height-based blending with zero transition distance can yield a fairly sharp transition between the two snow layers' materials. In any abrupt height transitions, this creates artifacts, since the transition height is only covered by one or two tessellated triangles, and the cutoff height of material transition can vary significantly in an obvious way. Increasing the height transition distance, even to a small value like 0.01 (1 cm) can help blend this transition. At view distances not extremely close-up, these artifacts may not be so noticeable. Note that increasing the height transition distance will tend to blend away the lower layer's material properties, so we can't increase it too much higher. Increasing the amount of blurring of the captured height map can also help, but comes at the cost of performance, particularly when capturing at higher resolutions.

We can do a similar thing to update the normal map as well. We'll create a Custom Render Texture to hold the updated normal map. What Color Format should we use to store a normal map? From the options, there's nothing that immediately jumps out as being specially suited. Through some trial and error, it turns out that we want to choose a format with unsigned R and G channels, holding a scaled version of the x and y normal values in the range [0,1], like R8G8_UNORM.

To update this over time, we'll need to create this normal map from a height texture. To do this, we'll need to take the derivatives of the heightmap and use that to reconstruct the normal map. We'll use these partial derivatives to construct the tangent vectors along the u and v coordinates of the texture, then take the cross product to get the normal vector. This is done, for example, in the [ShaderGraph implementation](https://docs.unity3d.com/Packages/com.unity.shadergraph@6.9/manual/Normal-From-Texture-Node.html). Note that there are many different ways we can try to approximate the tangent vectors from sampling neighbouring values of the heightmap. We'll use a first-order [central difference](https://en.wikipedia.org/wiki/Finite_difference). Note that we're trying to do this using world-scale values, so we'll need to take into account the world-space scale of the u and v coordinates (1.6 m), as well as the world-space scale of the heightmap (0.1 m), so that when we construct our tangent vectors, they are all done with the same scale in meters for all three components.

Let's create a new Shader and corresponding Material to update this Custom Render Texture, as we did before for the layer mask and height. Which height map should we generate our normals from? If we try to use the _combined_ heightmap, we'll immediately see that our normal vector approximation breaks down on the high-frequency details of the undisturbed sections of the heightmap. Contrast this with the results we got from Substance Designer, which at least produced a fairly usable result. However, for the same reason we [previously produced our normal map](#normal-blending) by blending the original normal map with the normal from the height mask, we'll do the same in our shader. The loss in detail of trying to calculate from the full heightmap is even greater this time. As a result, we need to access the updated mask height only. Note that we only have either the current frame's height, or the combined heightmap (original and mask together). We'll remedy this by simply creating a new Custom Render Texture just like our updated heightmap, but we'll initialize it to fully White, with no texture, so it'll contain the updated height mask over time.

We'll still need to combine the initial normal map with the one from the depth capture. Normal map blending is [somewhat complicated](https://blog.selfshadow.com/publications/blending-in-detail/), so it's not something we can accomplish with a simple blend operation, like taking the minimum for the height. We'll need to combine the two normals on every update of our Custom Render Texture.

To finish our Shader and Material, we'll ensure we can sample the mask height texture and original normal map. We'll calculate the mask normal as described above, and blend it with the original using reoriented normal mapping (see the [ShaderGraph implementation](https://docs.unity3d.com/Packages/com.unity.shadergraph@6.9/manual/Normal-Blend-Node.html) for inspiration). We also need to pack the resulting normal, a 3-component vector, into our RG-format Custom Render Texture. We'll take `0.5 * (normal + 1)` to transform it from the range [-1,1] to [0,1], and we only need to output the RG coordinates in the fragment shader. See the shader code for details. Finally, we'll assign the appropriate textures (Custom Render Texture height mask and original normal map) to our Material.

We can see that the normals of the compressed areas are now being updated as we'd expect, and respond to lighting accordingly. We can easily see the difference by looking at the normals in the Render Debug window. As with the layer mask update case, however, we also see an error occur when an object enters just below the 10 cm upper range, but the top snow layer's height is less than that. While the height is not updated, we see that the normals _are_. This is not surprising, since the Custom Render Texture we use to store the mask-only height is still being updated to show the presence of the object, and so will the normal map we derive from that height map. As a consequence, the normal map blending with the original top-layer normals will show this artifact.

Let's fix this issue by only showing the blended normal when the mask height is below the initial layer height. This means in our normal update shader, we'll need to sample the initial top-layer height map as well. We'll add that texture property to the shader so we can sample the initial height, and assign the map to the corresponding Material. We'll also add a height-based factor to interpolate between initial and blended normals, so that the values don't discontinuously jump where the mask height equals the initial height.

We also notice one final issue: the effect of the normal map seems particularly strong. That's because we're blending the initial normal map with the height-derived normal map, assigning this Custom Render Texture to our final snow material Layer 2's normal slot, _and_ setting a normal strength of 3. So, while the initial normal map will have the strength we want set, we'll also be incorrectly multiplying the strength of the height-derived portion. Therefore we'll need to set the snow material's normal strength to 1, and alter the initial normal map's strength in our Custom Render Texture update shader _just before_ blending it with the height-derived one. We'll add a new Range property to that shader, set the value in the corresponding material, and set the snow material normal strength to 1. To update our shader, we'll add a function to get the strength-altered normal. We can look to the [ShaderGraph implementation](https://docs.unity3d.com/Packages/com.unity.shadergraph@7.1/manual/Normal-Strength-Node.html) and [other discussions](https://computergraphics.stackexchange.com/questions/5411/correct-way-to-set-normal-strength/5412) for inspiration; we'll also ensure our final result is normalized before we blend normals.

#### Updating Layer Mask, Height, and Normals

Is there a way we can get the layer mask blending (for both height and surface properties) and also update the normal maps? As we've seen, direct update of the displacement map achieves similar results updating the layer mask in such a way as to achieve the same final height displacement. The differences come from the blending (layer mask based or height based) used by the snow material. Therefore, we could update the normal map directly for the layer mask approach, just as we did for the displacement map one.

However, let's take a moment to acknowledge a significant limitation of updating the snow layers' displacement or normal maps directly. When we do that, we lock the tiling of the snow displacement effect to that of the snow layer's textures! This has worked fine for our test setup, where the textures tile exactly once, but will not scale well to larger areas, where we would only want a single heightmap to be captured. Instead of doing the normal blending when updating our Custom Render Texture for the normal map, we really need to do it in the snow shader itself; the same is true for height blending. Thankfully, using our existing Material type, we can do exactly this. The height blending can be done using the layer mask like usual, which can be given a different tiling to the layer textures. The normal blending can be done using the Detail Map for the snow layers, which again can be given a different tiling. Note that this will preclude us using the Detail Map for its normal purpose, since it will not be providing smaller-scale variation, but instead the larger-scale variation from our captured height map.

We'll create a new Custom Render Texture to hold the detail map, with Color Format R8G8B8A8_UNORM, though we really only need the G (normal y) and A (normal x) channels. To create the detail normal map, we'll need to create a normal map from the height map; however, like before, we need to ensure that we only see an effect when the captured height map is lower than the top layer's height (as a fraction of the full range). We'll create a new pass in our normal map update shader, following a similar path of normal blending, using `float3(0,0,1)` upwards-pointing normal instead of sampling from the initial top-layer normal map. We'll create a corresponding Material and assign it to the new Custom Render Texture, taking care to assign the initial heightmap that has been remapping to the full height range (0-10 cm), and the mask-only height texture. We'll then assign the resulting normal map (remapped to [0, 1] range as before) to the G and A channels, as above, leaving the R (albedo) and B (smoothness) channels at their neutral values of 0.5. Finally, we'll need to make sure that we assign this detail map Custom Render Texture to _both_ snow layers, setting the detail normal strength to 1. We'll also ensure that each layer's Mask Map has a white B channel, so that the detail map will take effect everywhere.

With this, we can set up our snow material layers like usual, only adding Custom Render Textures for the Detail Maps as well as the Layer Mask map, all of which can be given a different tiling the layer maps. The end result is exactly what we aimed for: a layer mask that causes height transitions that match the captured height profile from the orthographic camera, and appropriately-altered normal maps. When we compare this to the approach altering the height and normal maps directly, using Height Based Blend, we see that the blending of the albedo is much more obvious here, and the lower snow layer makes a much stronger impact. Consider a point where the lower snow layer has a height of 1 cm, and the upper one has a height of 7 cm; a captured height map value of 4 cm would mean that the layer mask value would be 0.5, and equal blend of both layers, in height as well as material properties. However, we see half the albedo, etc. of the lower snow layer, even though it has a maximum height of 2 cm! Turning on 'Height Based Blend' doesn't help, since now the heights transition according to the Height Transition scale, not the appropriate falloff based on the height mask map. We can increase the Height Transition scale all the way up to 1, but this just ends up looking similar to when height blend is turned off, just slightly incorrect in the height values.

We can investigate something like this by adding a third layer, copying our second-layer settings to the third layer. In this case, the second layer is a new in-between layer with the same maps as the top snow layer, but the same height range as the lower snow layer. Note that we've decided to keep our main layer in this case, but we could also have moved the lower snow layer settings to the main layer, and kept only three layers total. Since this range is smaller than the top layer's, we'll reduce the normal strength down to 1. Next, we'll create a new Pass in our mask update shader, one that again uses height interpolation to determine the fractional blending. We'll still be using the same heightmaps for this interpolation, from the bottom (Layer 1) and top (Layer 2). The strengths for Layers 1 and 3 in the Layer Mask will be the same as what we were using for Layers 1 and 2 previously, namely 1 and _f_, where _f_ is the fraction from height interpolation. However, what we'll also do is use the fact that higher layers will take precedence over lower layers when not using height-based blending. For the larger values of _f_, we'll also want the blend fraction for Layer 2 to be 1 -- this will keep the material properties like albedo the same as the top layer until the height is reduced much closer to the bottom-layer height. Note that, even using the same height range, the height value of this blend will be slightly incorrect, since Layer 2's heightmap will differ from Layer 1's. At the small scale of the height range for Layers 1 and 2, we're not likely to ever really notice this difference, so we won't worry about fixing it.

Of course, all this becomes difficult because we are relying on the Layer Mask blending for both height and material properties; we have no other choice using HDRP's built-in Materials. If we were to write our own custom shader, much of these types of conversions and blending could happen in the shader directly. We will not cross that bridge yet, though.

### Orthographic Captured on Non-Flat Surface

Well begin relaxing the assumption of a flat base surface by introducing a mesh based on its own heightmap, similar to how a Terrain system might work. This gives us the most information to work with, because we can simply sample from the heightmap to determine what the height of the surface should be. From this, we can generate a mesh with the appropriate vertices, triangles, and normals, but we only need the heightmap to define the surface.

We won't use the Unity Terrain system for this, however. The Terrain system needs Terrain-specific shaders, such as HDRP's [Terrain Lit Shader](https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@7.4/manual/Terrain-Lit-Shader.html), which is simpler even than the regular Lit shader. Therefore, we can't easily use our LayeredLitTessellation Material. If we try, we'll get a notice saying that the material requires tangent vectors, which are not provided by the Terrain system. Rather than providing the tangent vectors on a vertex-by-vertex basis, the Terrain shaders calculates them based on the Terrain's underlying heightmap. When we decide to create a customized HDRP shader, we could take the same approach.

The classes `HeightmapMeshComponent` and `HeightmapMeshGenerator` handle the creating and calculation of a simple, evenly-spaced mesh (two triangles per square) from a given heightmap texture. Note that the texture must be read-enabled, and only needs to be a single channel. We'll use Substance Designer to create a simple example; anything with a smooth height variations will do. For more information on mesh generation from heightmaps, information and examples are readily available online; see, for example, this Unity-based [project on GitHub and associated tutorial videos](https://github.com/SebLague/Procedural-Landmass-Generation).

By significantly increasing the Offset of the displacement Height Range for the material's top layer, we see that displacement is happening normal to the surface, not strictly upwards. Since we expect snow to pile vertically on the ground, we'd like the displacement to be vertical. For such a small displacement range that we're using so far(10 cm), combined with relatively gentle slopes, we can get away with approximating this displacement as vertical.

#### Subtract Depth From Mesh Heightmap After Depth Capture

Our first approach will be to capture the depth within the entire volume covered by the mesh, subtract the mesh's height, then update the Layer Mask as usual. This is the simplest setup, because it only requires an additional Custom Render Texture for the subtracted height. We'll begin setting up the basic depth capture structures as before: a RenderTexture for the immediate height capture, a Custom Render Texture with associated Material for updating the layer mask, and an orthographic camera. Note that the camera must have a far clip plane that captures the mesh's maximum height (we've chosen 1 m) with the snow material's maximum height (as before, 10 cm, for a total of 1.1 m). Next, we'll set up the subtraction Shader, which will sample the depth texture and mesh's heightmap, scale each according to their maximumranges (1.1 and 1 m respectively, in this case), subtract them, then divide by the range difference (same as our snow's height, 10 cm), and clamp between zero and one using `saturate`. Since we'll want this CustomRenderTexture to take the place of the original depth RenderTexture, we need to ensure it is not mirrored or flipped with respect to the originally-captured RenderTexture. We'll create a corresponding CustomRenderTexture and Material, and assign appropriate references.

We'll need to ensure that the amount of blurring of the captured depth texture is kept to a minimum. The reason for this is that the blurring is happening before rescaling and subtracting, which is not equivalent to blurring the height above the mesh surface. With this change, the setup works, at least in principle. There are several additional issues with this approach, however. We're losing a lot of the precision of our depth buffer, because we're captured values in a large height range (here 1.1 m), but at any given pixel only care about the variation within the snow height (here 0.1 m), a small fraction of the range of our texture.

Additionally, because we always capture all objects within the full volume of the mesh, we capture objects that are even _fully_ below the surface, but still within the bounding box of the mesh. This was not the case for the flat surface, where those objects would be outside of the orthographic camera's frustum completely, and thus never rendered at all. These objects will show up with a lower height than the surface, so when we remap to the range of snow height above the surface, they will be negative, and therefore clamped to zero. With depth clipping turned off, this will be even the case when the object is partially below the mesh bounds.

The question becomes: is this even an issue? In most landscapes, we don't expect to have any below-ground geometry, and we expect all objects to be above the surface, controlled using physics (eg. for moving rocks) or inverse kinematics (eg. for feet). we may run into slight clipping issuse where some parts of objects will dip below the surface bottom, but in those cases we still want to treat those as depth-zero, so this implementation will achieve what we want. More work will need to be done if we wish to have underground geometry such as caves. Note that it may be easiest to use something like the Layer system and keep track of which objects are above or below ground (perhaps using Trigger Colliders or similar), which lets us simply avoid rendering below-ground objects in this custom pass. 

We have options if we wish to avoid writing zero depth for fragments underneath the surface. One would be the `clip` method, which can discard the current fragment, or we could simply conditionally set the final depth value to 1, which will achieve the same thing due to z-testing. However, _any_ parts of objects below the surface will no longer have an effect on the snow. If we have a reasonable idea about the expected height range of this type of clipping, these could be reasonable approaches to apply for objects a certain distance below the surface (probably on the order of the snow height, around 10 cm), which could allow objects far under the surface to be safely ignored even within the subtraction shader. However, since we are blurring the captured depth before subtracting, this type of approach will have some artifacts at the edges of objects, which will appear in the depth map to be higher than the surface due to blurring.

Finally, there is a general issue that our surface positions (in particular, the height) are defined by linear interpolation of the mesh's triangles, while we are doing biliear sampling of the mesh's heightmap to determine the surface height at any given point. By increasing the number of divisions, we can minimize the discrepancy between these two approaches, but adding all this extra CPU-side geometry has its costs.

Another factor to note is that we have is that we've chosen Phong tessellation mode. This will cause the resulting tessellated mesh to be smoothed a little bit. We can see this by reducing the number of divisions in the mesh to something small (around 10) and increasing the height, so that the underlying triangular structure is very obvious when we turn Tessellation Factor down to 0. We'll use Scene Wireframemode to visualize and set Displacement Mode to None, so we are only seeing the tessellation. Increasing the tessellation factor, we can see that the sharp peaks are smoothed out. Setting Tessellation Mode to None, we instead see that the additional geometry (without displacement being applied) lies exactly within the base mesh's triangles, without smoothing. This further complicates the relationship between the heightmap lookup and the actual height of the geometry.

We'll finish this setup by creating the custom render textures for the mask height and detail normals, the detail normal update material, and making the appropriate material assignments for the CustomRenderTextures, as well as to our snow Material.

#### Subtract Depth From Mesh Heightmap During Depth Capture

Next, we'll try to get around some of the precision issues by performing the subtraction within the same shader used in our depth capture pass. To do this, we'll need to create a new custom shader, which we'll base off of an regular (non-HDRP) Unlit shader for simplicity. Like HDRP's [Unlit Shader](https://github.com/Unity-Technologies/Graphics/blob/master/com.unity.render-pipelines.high-definition/Runtime/Material/Unlit/Unlit.shader), we'll set the `LightMode` to `DepthForwardOnly`. Our vertex structure will need `float4 clipSpace : TEXCOORD0` in addition to the usual `float4 vertex : SV_POSITION`. We'll copy the vertex value so it can be interpolated normally when we get the value in the fragment shader, giving true clip-space position in [-1,1] range (xy) and [0,1] range (z), rather than the kind of screen-space psuedo-clip-position that Unity ends up setting (you can verify this by inspection). The fragment shader will look similar to that of our depth-texture based depth subtraction one. The difference is that the depth will be obtained from the clip-space z coordinate, instead of being sampled. The UV coordinate for the heightmap sample will be obtained from clip-space xy, remapped into [0,1] range. One final change is that our [output semantic](https://docs.unity3d.com/Manual/SL-ShaderSemantics.html) will be `SV_Depth` instead of `COLOR`. We only need to write to the depth buffer, so just this single output from the fragment shader will suffice. Note that normally we don't need to write explicitly to the depth buffer, since it usually just comes from the fragment's clip position. By adding this semantic, we'll write it explicitly, though at a performance cost. Note that we'll continue to output to the depth buffer, instead of using a shader that writes to a color texture, since our bake custom pass is already set up to write to a depth RenderTexture. Since we're rendering in UV space, we need to consider whether UV space is flipped vertically by checking the [`_ProjectionParams`](https://docs.unity3d.com/Manual/SL-PlatformDifferences.html) built-in variable.

To actually use this Shader in our custom pass, we'll expose a Material property and set the `overrideMaterial` in the `RendererListDesc`. Finally, we'll need to make sure that our non-HDRP shader has the correct information that it needs. Using the Frame Debug window, we can see that HDRP materials look for the Matrix property called `_ViewProjMatrix`, while the one we've created looks for `unity_MatrixVP`. In both cases, this is the View-Projection matrix. We can see that in our custom pass, the example we derived from also set this matrix under two different names using `cmd.SetGlobalMatrix`, where `cmd` is the command buffer executing the pass. We just need to do the same thing additionally with `unity_MatrixVP`. We'll create a Material with this new Shader type and assign the heightmap texture reference.

We'll set up the Custom Render Textures and associated materials as we've done for our flat implementations (we won't need the extra CustomRenderTexture just to do subtraction). The main difference is assigning the depth subtraction and write Material to the depth bake CustomPass.

We can see similar results, though we may increase the depth pass's blur, since by performing the subtraction in the initial capture, we are blurring the height above the surface. However, we can't increase it back to the value we used in our flat implementations. This blurring does help, but we can clearly see that the discrepancy between the sampled heightmap and the actual geometry's height is still a limitation, unless we create an initial mesh with very dense triangles. This is likely also the reason why we must still limit the amount of blurring compared to the case of a flat surface.

Here, we'll implement the `clip` operation in our fragment shader, feeding in the subtracted depth (in world units) plus the snow's height. This extra buffer will allow for objects to clip slightly below the surface, and also give extra space to accomodate differences between the sampled heightmap and geometry height. This way, we can still achieve fully-compressed snow without artifacts, while ignoring contributions from objects significantly below the surface.

#### Vertex Height Lookup Texture

With a Tessellation Mode of None (without Phone tessellation), we know that pre-displacement surface height is the same as from the base mesh; that is, linearly interpolated within each triangle from the barycentric coordinates. To replicate this, within our depth subtraction-and-write shader, we'd have to sample the heightmap at corresponding locations for the closest vertices, and perform the same barycentric interpolation. We could store the height at N x N verties in an N x N sized lookup texture, for example. Note that it's not correct to perform bilinear interpolation between heightmap values where the mesh vertices are placed, since this interpolates between the _four_ closest vertices, not the three of each triangle.

This is a lot of work to effective replicate the process of simply rendering the surface's depth. Furthermore, handling changes in height due to Phong tessellation makes this an even more intricate process. For these reasons, and the inflexibility of requiring the surface to be generated by heightmap, we won't pursue this technique further.

#### Subtract Ground Mesh Depth Within CustomPass

The most direct and accurate way to determine the ground surface's height is to render it with the orthograpic camera, as we've been doing for the other objects. We only need to render the depth, so it's cheaper than rendering a lit surface, but this is extra efforc compared to simply re-using the heightmap. This lets us use any mesh as the ground surface, not just one generated from a heightmap. For this gained accuracy and flexibility, it's worth it to need to render an additional pass for the surface.

We'll extend our CustomPass with a boolean option to capture the ground depth. We'll use a `LayerMask` do identify objeects that comprise the ground, and one to identify objects whose height relative to the surface we want to capture. These should be set as the `CullingMask` in the `ScriptableCullingParameters` of the CustomPass. It should not be set in the RendererListDesc, whose `layerMask` is a RendererLayerMask, and completely separate from the Unity GameObject Layer; see [this tutorial](https://catlikecoding.com/unity/tutorials/custom-srp/multiple-cameras/) for more details.

In the CustomPass, we'll need to allocate a depth buffer to hold the result, as well as create the appropriate Material for depth subtraction when rendering the non-ground objects. We'll need to create a new Shader for this, which will be very similar to the previous depth-write subtraction shader, except we'll sample from the depth texture (not heightmap) and we don't need to mirror horizontal uv coordinates, since we'll be rendering from the same camera point of view both times. In the CustomPass, we'll also need to set the new Material's ground depth texture and height ranges. For now, when using ground depth capture, we'll keep the heightmap depth write material assigned, but only use it to determine the height ranges.

The setup for capturing the ground depth with `HDUtils.DrawRendererList` is similar to that for the non-ground pass. It has the different CullingMask, and the `RasterState` needs to be different; we no longer want to turn off `depthClip`, but instead want to set `CullMode.Off` so that the upwards-looking camera can capture the backface of the ground.

We'll also set the `GreaterEqual` depth comparison function' in the `DepthState`; since the orthographic camera looks upwards, the furthest surface will be the highest, though for now, we expect to see a single ground mesh. To do this, we'll use the command buffer's `ClearRenderTarget` function to set the clear depth to zero, instead of the default of 1. Clear values not written by the surface will not be accessed, expect potentially at the edges of a mesh due to blurring of the final result; in any case, a default value of minimum height, instead of maximum, is at least as correct.

We also notice that any of the HDRP Tessellation materials will not show up in our CustomPass, though if we use another Material, such as HDRP Lit, they will. We're only trying to capture the undisplaced surface height, so this is mostly sufficient; however, it won't capture the smoothing if we have Phong tessellation enabled. In our CustomPass, we'll create a new HDRP Lit material to use as the `overrideMaterial` when capturing the ground height.

If we set Tessellation Mode to None on our snow Material, we see that this method indeed very accurately captures the height of objects above the surface (we can debug this by turning off Displacement and setting obvious color tints for individual layers). However, the difference between Phong and None tessellation modes becomes more obvious when the base mesh is not very dense. Note that in those cases, Phong tessellation can really help smooth out the underlying surface geometry, so we may expect that this scenario could come up.

TODO: We'll need to determine how to ensure that (Phong) tessellated materials (such as HDRP Lit) properly show in the depth capture of the CustomPass.

#### Other Approaches to General Surfaces

There are other approaches we could consider to handle objects partially below the ground height. One key point is that we need to distinguish frontfaces and backfaces in how we consider determining the space occupied by watertight meshes. If the lowest above-ground point is a frontface, then that is the lowest occupied height; however, if it's a backface, then the frontface must be below ground level, and so the snow height at that point must be zero! There are several possibilities for determining this information. We could perform separate passes for frontfaces and backfaces, writing difference reference values in the [stencil buffer](https://docs.unity3d.com/Manual/SL-Stencil.html). Alternatively, we could use the [`VFACE` semantic](https://docs.unity3d.com/Manual/SL-ShaderSemantics.html) to determine the facing dirction of a given fragment, but we'd likely need to set a separate render target for this information, since using a stencil reference semantic like [`SV_StencilRef`](SV_StencilRef) seems platform dependent. In any case, we'd need to discard any fragments below ground level, and then perform a final pass that sets the height to zero any pixels that were indicated as backfacing. This type of solution could be useful in projects where underground areas (such as caves) need to exist below the ground height, but still within the capture volume of the orthogrphic camera.

### Automated Setup

Setting up the RenderTexture and Custom Render Textures, their associated materials, and assigning them to snow material layer properties, while instructive, is not the best workflow.

Let's try to automate most of this setup. For a single ground Renderer, as we've been using so far, this is fairly straightforward, but we'll try to accomodate a set of MeshRenderers comprising the ground. The most difficult part will be handling the appropriate UV mapping. This is true for the displacement maps (layer mask and detail normals) as well as the snow layers' UVs, since we want the snow layers to be consistent across ground objects.

#### World-Space Alignment

We'll begin by creating a new CustomPass to handle the setup in the simplest case.

In our previous setups, UVs were distributed linearly across horizontal components of a square or rectangular mesh. The orthographic camera was set up to cover exactly this horizontal rectangle, and therefore the baked depth texture could be applied directly through these UVs. What if we have multiple objects that comprise the ground? We need to map the portion of the height-generated textures to these objects, so for now, we will require a linear UV set as well, so we can use texture scaling and offset to perform this mapping.

The most straightforward setup will be ensure all objects have linear UVs and are aligned to world axes. Then, we can simply determine the entire world-space bounds of all the renderers, and use this rectangle to define the frustum of the orthographic camera, and thereby the mapping of the height-generated textures. This is similar to using a Planar uv mapping, though that option is not available to detail map in our material, so we muset set up the textures' offset and scaling ourselves to achieve the same effect.

We'll create `SnowDepthBakeCustomPass`, which we'll base off of `CameraDepthBakeCustomPass` (we won't subclass so we can keep `CameraDepthBakeCustomPass` in its current state for learning purposes). As input, we'll take in a root GameObject; we'll check for child MeshRenderers that could be ground objects. We'll also take in a Layered Material that we will use as the template for all the snow (non-main) layers; we'll assign those properties to ground objects so they can have their different main layers, but consistent snow layers on top. We'll also input the maximum generated texture width or height, a LayerMask defining ground objects, one defining the snow-compacting objects, and the depth buffer blur radius.

To ensure that the detail maps will be accessed, we need to make sure that the template material has a dummy neutral (grey) detail map set for all of the snow layers. We'll need to do this also for all snow renderers materials. This is a bit of additional setup, but it helps remind us that the detail map slot is used by the snow baking custom pass, and prevents us from needing to create and modify the snow renderers' materials at runtime (enabling detail map keywords).

Since we can still create Custom Render Textures in this setup, we'll use those to schedule the passes for layer mask height, layer mask, and detail normal textures.

Let's sketch out the actions we'll take on `Setup` of this custom pass. The parts that affect the renderers in the scene will only happen during play mode, which means the effect won't work in the scene view, but we save potential confusion.

* Validation
** Validate the inputs, to ensure that the ground/snow mesh renderer root is set, and that the template material has the appropriate properties.
** Clear lists to hold all ground/snow mesh renderers, and lists to hold temporary MeshRenderers and Materials.
* Determine Renderer Bounds
** For all MeshRenderers under the root, determine if they are a ground/snow renderer (match template material and keywords, ensure uvs are linear and world-space aligned).
** Determine the encapsulated world-space Bounds of all snow/ground Renderers.
** From the renderer bounds, use the maximum texture size to determine the generated texture sizes, and the template material height ranges to determine the orthogonal camera far clip plane.
** Use the renderer bounds and far clip value to create and place an orthographic camera, with the appropriate aspect ratio, at the bottom-center of the bounds, facing upwards, to capture the axis-aligned volume surrounding the ground renderers.
* Depth Pass Setup
** Find shaders and create materials for blur pass and depth subtraction pass (after ground height pass).
** Allocate RTHandles for these passes.
** Set ground texture and height ranges to depth subtraction material.
* Custom Render Texture setup
** Create a layer mask material, taking in baked depth texture, top and bottom snow layer heightmaps, and their fractional height ranges.
** Create a detail normal material, taking in initial height and normal maps, initial normal strength, and world scale from bounds size and template material snow height range.
** Create a Custom Render Texture for the masked snow height, using the mask material and HeightUpdate pass. Assign this as the height texture in the detail normal material.
** Create a Custom Render Texture for the layer mask, using the mask material and MaskInterpolateUpdateGreen pass if three total layers (MaskInterpolateUpdate_BetweenLayersOneAndTwo if four, so three snow layers where we're treating the middle as a lower version of the top one).
** Create a Custom Render Texture for the detail (normal) map, using the detail normal material and DetailNormalUpdate pass.
* Set up changes to snow Renderers if in Play mode, by applying a MaterialPropertyBlock
** Copy *all* properties for layers 1, 2 and 3 (as appropriate), as well as many of the blending settings, from the template texture to the MaterialPropertyBlock. See `SnowMaterialUtil`.
** Set the Layer Mask map and Detail Normal map (for layers 1, 2, and 3 as appropriate) from the Custom Render Textures just created.
** Set the detail mask uv mapping channel to UV0, disable locking to base tiling, and set detail normal strength to 1 (and other details strengths to 0).
** Get the scaling and offset of the Layer Mask and Detail Normal textures from the fractional horizontal size and the fractional bounds minimum of the renderer's world-space bounds within the fully-encapsulated world-space bounds. Set `_LayerMaskMap_ST`, `_DetailMap1_ST` (and other layers') from the scale-offset Vector4.
** From the given template material scale (in m), determine for each Renderer the offset and scaling of each texture for the snow layers, so that these textures tile at the given scale in world space. Additionally, they will tile seamlessly across Renderers.

The `Execute` function is mostly the same as for `CameraDepthBakeCustomPass`, though we will always determine the ground height by performing the generic ground height baking pass. In `Cleanup` we make sure to `Release` the Custom Render Textures and destroy any created materials. One difference is that we store the ground height in a permanent RenderTexture (not a temporary one allocated by `RTHandle` system), and only need to perform the pass when required; usually, this is just once after the Setup, since in most scenarios the ground height won't change at runtime.

We'll set up an example with several `HeightmapMeshComponent` objects, since at no rotation, the resulting MeshRenderers satisfy having the required linear uvs in world space. While this is a serious restriction, we note that we're able to create a complicated multiple-mesh ground height structure that is correctly baked by the ground depth pass. The key point is that there is still a one-to-one relationship between horizontal position and ground/snow height (that is, no overhangs or similar).

#### Rotated World-Space Alignment

We have a little more flexibility in our uv mapping than strict world-space alignment. While we've been assuming that the u coordinate runs along the x-axis, and the v coordinate along the z-axis, any two horizontal directions could work equally as well.

For simplicity, we'll use the snow objects root to hold any potential rotation around the vertical axis. While in the `Setup` for our custom pass, we'll store the initial local rotationvalues, set them to identity, and perform the rest of the setup. In this way, we assume that the unrotated setup *is* world-space aligned, as before. We'll also parent (temporarily) the baking camera to the root, after positioning the camera. Then we'll reset the root's local rotation value, and unparent the baking camera.

Using parenting this way may be slightly less efficient than performing the matrix-based transformations ourselves, but the cost is only once during setup, and the resulting code is much simpler and easier to understand.

This method indeed allows us to rotate our root object so that our linear uvs do not need to be aligned to x and z world axes. Note that this method will also allow rotation around other axes than the vertical y axis, potentially leading to unintended behaviour, since then the 'up' direction (as seen by the baking camera) will not be the world y (up) axis.

Since we require a strict level-building setup with linear UVs, there's not much upside to doing a 2D oriented bounding box fix to the final position of the renderers.

#### Height Falloff at Edges

With our automated setup, we can handle a mixture of different meshes that comprise the surface on which snow will be layered. They must maintain the one-to-one relationship between horizontal position and ground height, so this setup cannot currently support overhangs. However, it _does_ support sudden jumps in height, such as a box being placed on the ground (if the top and side surfaces are appropriately differentiated between Layers, and have appropriate uv structure). 

However, at the edges of objects, where there is a discontinuity in height, we expect the snow not to be able to pile exactly at the edge of the object; instead, we may expect the snow hieght to diminish close to the edge. By trying to detect the edges in the generated ground height texture, we can impose this kind of falloff to give a slightly more natural profile of snow at these edges.

Note that there is another technical reason for wanting to taper the snow height to zero at object edges. The reason is that the height displacement of our tessellation shader will be visible when looked at from the side, creating a hole relative to the top of the associated 'wall' geometry covering the height difference. Side triangles from the original height are _not_ generated by the tessellation. Therefore, but having the height increase from zero at the edges, our snow surface will start with no gap, and naturally create a side surface as the snow reaches its full height.

We will generate this falloff texture once, along with baking the ground height, and use it as a constant minimum height. To do this, we'll use the [Scharr filter](https://en.wikipedia.org/wiki/Sobel_operator#Alternative_operators) as a rotationally-invariant approximation to the height gradient. We'll use the corresponding gradient length as simple edge detection; where the gradient is large, we'll reduce the minimum height. To prevent high-frequency detail from creeping into our results, we'll downsample the ground height first. Since the derivative will only be large directly at the edge, we'll blur the results so the falloff can spread across multiple pixels of the height map texture. Finally, we'll remap the blurred results to ensure that locally-minimum values are mapped to zero height. We'll do this by, _in parallel_, performing erosion in the same area as the blurring, thereby capturing the local minimum falloff height. Then we'll remap the blurred results, taking the range [local minimum, 1] to [0, 1], and take the result to a low power, to try to reach a natural-looking height profile.

We'll also use the UV values to create a falloff around the edges of the falloff texture itself, since the edge between geometry inside and outside the orthographic camera's area won't be captured by this approach. This way, we ensure that the snow height will fall off to zero at the edges of the area of consideration, so we don't create cracks there due to height displacement.

The results are fairly reasonable at the top edges of high objects. There are a few obvious consequences of our gradient-based approach. One is that very steep portions of the ground will naturally have a large enough gradient to be considered an 'edge', and thus have a reduced snow height. Additionally, at the edge of the ground below a jump in height, the snow height also falls off, when, if anything, we may expect additional snow to pile up there. To begin to manage this, we would need to determine which parts of the height map correspond to areas below an edge or above an edge. Unfortunately, the map we've constructed does not effectively partition the space into definitive edges, so we don't have a straightforward approach to generating the 'direction to closest edge' that could determine high and low sides of an edge.

Nevertheless, we can roughly approximate locally which side of an edge we're on by the sign of the second derivative. Let's consider a one-dimensional edge given by a [step function](https://en.wikipedia.org/wiki/Heaviside_step_function). The step function is just an extremely steep limit of a [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) like the [logistic function](https://en.wikipedia.org/wiki/Logistic_function), which is the kind of shape we'd expect to see after blurring the step function. We can see that the derivative is maximum where the step originally was, implying that the second derivative is zero there. The second derivative is positive on the lower side of the edge, and negative on the upper side; this still holds if the edge function is reversed along the x-axis.

Indeed, using the second derivative can help narrow down the edge location, since edges are located where the second derivative is zero. Note that this criterion alone is not sufficient, requiring also a large derivative; in relatively flat areas, both first and second derivatives will be small, and the second derivative will be zero at the inflection points, even though there are no true edges.

We'll use the [Laplacian](https://en.wikipedia.org/wiki/Laplace_operator) when generalizing this second derivative to more than one dimension, which similarly acts as a source of the original function when positive, and a sink when negative. In our case, we wish to consider the side of edges where the Laplacian is positive (lower side) and remove any edge falloff there. We can do this by mapping the sign of the Laplacian to [0, 1] and taking the maximum with the falloff texture; in effect, only applying the Laplacian sign in areas where an edge was determined to exist. We'll use the [nine-point discretization](https://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm) of the Laplacian, acting on the blurred downsampled height. Unfortunately, it's obvious that the resolution of the map and its bilinear filtering causes this to take effect even on the upper side of the edges. By eroding the sign of the Laplacian, even just by one pixel, we can avoid this artifact. The result is a somewhat subtle decrease of the falloff on the lower side of edges, but it's a reasonable step that we're able to achieve using only blur and derivative filtering of the heightmap.

One other approach we could consider is to render the object IDs from the orthographic camera's point of view, which could help us determine the edges between objects. However, we'd still run into the same resolution and aliasing issues, in that edges in the geometry wouldn't line up with pixel edges in the lookup textures, so there could always be bleeding from one side of an edge to another. As a result, we'd need to consider a different architecture, beyond just a single snow height texture covering the entire area, to handle edges with greater precision.

#### Smoothing Heightmap for Finding Normals

So far, we've used the simple directional partial derivatives of the height mask texture (sampling previous and next texels) to construct the detail normal vector. Such an approach can yield noisy results. We may opt to use a smoother result by using something like the [Scharr filter](https://en.wikipedia.org/wiki/Sobel_operator#Alternative_operators) as a rotationally-invariant approximation to the height gradient. Such a filter is separable, for the x and y components, into a blur along one direction and a centered derivative along the other. Since our normal update shader takes a centered derivative approximation already, we'll just need to provide blurred version of the mask height texture along each direction, sampling each as appropriate for the gradient direction.

This approach starts to have a significant effect if the heightmap has a lot of high-frequency detail, adding extra consistency to edges.

Note that our current implementation involves using a temporary gradient texture, and since it's updated at the end of our CustomPass, introduces an extra frame of latency between when the final mask height texture is updated and when the detail normal texture is updated. While we could create another CustomRenderTexture to store these results, thereby updating along with the other CustomRenderTextures, this would increase our memory footprint, and will not be helpful later when we wish to update the mask height texture using a ComputeShader, instead of the CustomRenderTexture as we do currently.

As a side note, it seems that we need to choose the inverted y direction when determining the bitangent vector to generate the normal map from tangent and bitangent vectors from the heightmap, as though to create a left-handed coordinate system where (x cross -y) yields z. This indeed creates the correct resulting normal vectors, as we can check in the Render Debug window under Material > Lit > Normal View Space. Strangely, the resulting detail normal texture looks like the Y channel is inverted to what we'd expect from a similar authored/baked detail map assigned in the Material inspector. We'll leave this as a user-configurable property of the CustomPass, and return to this issue later if any corresponding issues arise.

### Future Placement Work

* Determine how to ensure that (Phong) tessellated materials (such as HDRP Lit) properly show in the depth capture of the CustomPass.
* Perform normal-direction, instead of vertical, displacement.
* Perform planar mapping of snow layer/height/normal blending based on placement of orthographic capture rectangle (see [future shading work](#future-shading-work)).
* Automatically determine best-fit horizontal rotated rectangle to fit the ground.
* Determine appropriate architecture for handling or excluding below-ground objects when baking height and motion vectors.
* Consider adaptive mesh-based displacement (instead of tessellation on top of existing layered surface).
* Determine appropriate architecture for handling multiple objects and corresponding jumps in height.
* Determine appropriate architecture for handling setup of snow custom passes when concave objects are on top of the ground (more than one surface at the same horizontal position).
* Determine appropriate architecture for automatic or adaptive setup to change captured texture sizes based on performance requirements.
* Consider improvements to blurring of per-frame depth capture texture, to give a better height profile of snow around the edges of objects.
* If desired, determine appropriate architecture for handling runtime changes to setup-dependent properties.
* Determine if no objects are capturing by baking camera for a given frame, and if so, skip unnecessary passes.
* Test for correct handling of [platform-specific](https://docs.unity3d.com/Manual/SL-PlatformDifferences.html) rendering in UV space, clip space coordinate conversion, and depth direction for all shaders across platforms and graphics APIs.

## Version Update

### 2020.3 Changes

We will update to use Unity 2020.3.6f1 with HDRP 10.4.0, so that we can take advantage of correctly-rendered motion vectors when we perform our simulation.

Fortunately, none of our material, shader, or Custom Render Texture setup needs to change. 

Note that we can set any Injection Point for our Custom Pass Volume, so we can choose Before Rendering. As a result, we'll revisit our use of CustomRenderTextures, instead performing all the updates within this CustomPass using `CoreUtils.DrawFullScreen`, so that results are available in the same frame. To do this, we'll need to make another version of our layer mask and detail normal shaders that use the CustomPass vertex shader setup, instead of the CustomRenderTexture one.

The only API change we need to consider is a small one in `CustomPass`. First, we must update the `Execute` function; the new API combines all parameters into a `CustomPassContext` instance.

Second, we still can (and need to) set the `unity_MatrixVP` value via the command buffer, so that our depth write shader (which uses default behaviour for vertex transformations, not those of the scriptable render pipelines) will work as before.

Third, we must change how we perform baking to our non-rendered orthographic cameras. We'll change from the deprecated `HDUtils.DrawRendererList` to `CoreUtils.DrawRendererList`. However, much of the updating of camera parameters now happens under the hood in HDRP, so we cannot simply set those values directly via the command buffer. With [`CustomPassUtils`](https://github.com/Unity-Technologies/Graphics/blob/f68f31c838aab006080ec196f12a856e868033f5/com.unity.render-pipelines.high-definition/Runtime/RenderPipeline/RenderPass/CustomPass/CustomPassUtils.cs), we could use `RenderWithCamera` or similar functions. However, to more directly keep the behaviour of our `DrawRendererList` calls, we'll keep those, but use the `HDRenderPipeline.OverrideCameraRendering` struct, which sets the appropriate camera parameters.

Note that this struct is currently internal, and will set the baking camera's aspect ratio to match the rendered camera's. We'll need to make slight modifications to the HDRP package to make it public and give the option of not changing the aspect ratio. 

### HDRP Package Modification Instructions

To use a modified version of the HDRP package, simply copy the package (from Library/PackageCache) into the Unity Project's Packages folder, where it will be found as an embedded package, and used instead of the cached version. We can safely make project-only changes to the source code of this copy.

Specifically, we'll look for the `Runtime/RenderPipeline/HDRenderPipeline.cs` file, and the `OverrideCameraRendering` struct. We'll begin by making it `public` instead of `internal`. Next, we'll note that it has one constructor, taking `CommandBuffer cmd` and `Camera overrideCamera`. We'll add another parameter, `bool matchAspectRatio`. We'll need to re-add the two-parameter constructor, calling the three-parameter version with `matchAspectRatio` as `true`. Finally, in the three-parameter constructor, we'll look for the calls to `overrideHDCamera.OverridePixelRect(...)` and `overrideCamera.aspect`, enclosing them in an `if` statement that checks if `matchAspectRatio` is true.


## Simulation

We'd like to simulate some of the interaction snow has with the environment. While the full problem would amount to a complete physics simulation, we can tackle this problem by splitting it up into elements of hierarchical importance. We need to consider both the compacting and movement (displacement) of snow. Major elements are:

* Effect of external object movement on snow
* Movement of snow over time
* Effect of snow movement on external objects

The representation of the snow, combined with its interaction with the rendering and physics systems, will affect how well these elements can be modelled.

Treating snow as a surface allows for straightforward integration with the rendering system, but limits the modelling that can be done.

More complicated treatments, such as treating snow as particles, allows more complicated modelling.

### Overview of Assumptions

Our current approach, in which object height is baked and then used set the snow height, makes _several_ simplifying assumptions. We'll revisit those assumptions here to understand how we can approach a more accurate or wholistic physical simulation.

* Snow is compacted by any object found above the ground and below the snow.
** Snow is compacted immediately, without velocity or acceleration
** No force is applied back to objects
** Snow is compacted permanently, even if the object leaves
* Snow is compacted based only on object height
** Object velocity is not considered
** Sideways movement through snow will still cause it to be compacted
* Snow layer takes only a single height
** Objects cannot move through or under snow

There are two main technical reasons for these assumptions:

* Treating snow as a surface
** Rendering as a surface with a single height value, not with multiple heights (supporting holes, folds, etc) or with a 3D volume
** Capturing the object state with a single height field representing the lowest point of the object, by rendering height with an orthographic camera
* GPU-centered approach no interaction with physics system
** Physics of snow height or positioning are not considered
** Snow height or existence does not slow down or otherwise affect objects
* Speed: Only frame-by-frame positions are captured
** Swept volume between previous and current position is not identified
** Large per-frame movement relative to object size will leave gaps in snow height field

Of these, interaction with the physics system is the potentially more complicated case. If the simulation happens on the GPU, we'd need to get data from the GPU back to the CPU, where it can feed into the physics system, and vice-versa. For simulation on the CPU, we'd need to updat the GPU representation with the result of the simulation. In both cases, this involves transferring from its structure in the GPU, back to the CPU, and then to the physics system, and back. Not only is transferring data between CPU and GPU costly, but the underlying representation is very different between GPU and physics system.

Also challenging is dealing with animation, inverse kinematics, and other edge cases where physical interactions are usually treated kinematically (as with floors or walls that don't move). Although Newton's third law tells us that force should be acting on both objects in an interacting pair, often it's most straightforward ot only apply force to the less massive object, ignoring the smaller effect that the force will have on the more massive object. For ankle-height snow, we often will be considering the limit where we consider the force on the snow, but not on anything falling into the snow. For very deep snow (say, 1 meter deep), this is clearly no longer the case! A person will not simply fall through to the ground below, compressing all the snow beneath them.

Without this link back to the physics system, any object that moves within the snow's volume _cannot_ have its movement affected; from the perspective of the snow, its movement is kinematic. Objects must therefore completely compact or move snow from the volume they inhabit. For examnple, stepping on snow may compact most of it downwards, but also move some of it to the sides. In contrast, an object slowly moving horizontally through the snow may mostly displace the snow out of the way, compacting relatively little of it. It is precisely this secenario that we will focus on for the most straightforward of our snow movement simulations.

### Simple Displacement From Motion Vectors

[Motion Vectors](https://docs.unity3d.com/Packages/com.unity.render-pipelines.high-definition@10.4/manual/Motion-Vectors.html) capture the per-pixel, screen-space motion of objects, in a similar way that our depth pass captures the per-pixel height of objects. While normally rendered from the point of view of a rendered camera, we can render them from the point of view of our orthographic cameras. By capturing the per-pixel motion like this on the GPU, we can achieve the same level of flexibility as with our height baking pass. This will, however, share the same downside that it can only capture motion for a single height value (the one closest to the ground) for each horizontal position.

We note that motion vectors tells us about the motion from the object's position in the previous frame. We can use this to determine the volume of snow that the object moved through, and the directions it was displaced in.

#### Baking Motion Vectors Texture

With our version update to Unity 2020.3 with HDRP 10.4, we can use `DrawRendererList` with `HDRenderPipeline.OverrideCameraRendering` to set values from our orthographic baking camera. We need to specify the `MotionVectors` pass name for the `ShaderTagId` and ensure `excludeObjectMotionVectors` is false when creating the `RendererList`, setting the `rendererConfiguration` to `PerObjectData.MotionVectors`. Finally, we'll need to bake to the appropriate type of texture. We need signed floating-point values in the RGB channels, so we can allocate a temporary texture or RenderTexture of colorFormat `GraphicsFormat.R16G16B16A16_SFloat`.

Note that, like the depth texture, this will be baked from the point of view of the upside-down orthographic camera, so (with our setup) we'll need to again mirror the texture x-coordinates (and x-value of the motion vector) to convert to the coordinate frame that the snow material will be using. 

Finally, we note that values may be positive or negative, and are stored in terms of frame-to-frame motion in clip space, in the range [-1, 1]. Therefore, it is straightforward to divide by two and get the change in uv position from the last frame. Note that only the xy clip-space values are provided, so we cannot get any depth motion information.

#### Simple Horizontal Displacement: Amount and Direction

Having access to horizontal motion vectors, we'll begin with a simple approach that considers only these horizontal components of the motion. We'll also only consider the displacement effect of this horizontal movement.

In this scenario, it is not only the direction of motion that determines how the snow will be moved, but the horizontal surface normal of the edge of the object. Consider the case of a snowplow whose shovel is oriented 45 degrees relative to the road. We must consider the [normal force](https://en.wikipedia.org/wiki/Normal_force) of the surface that prevents the snow from passing through the shovel; this is what moves the snow. Movement of the surface _tangential_ to the surface would only move the snow through their combined friction, which we'll ignore for now. Movement directly _away_ from the surface will push the snow the same distance, assuming no compression, since the normal force will never accelerate the snow faster than the object's movement, and the snow does not pass through the object's surface.

We'll therefore treat the motion of the snow as being the component of velocity of the surface that's normal to the surface. At each point on the horizontal surface of the object in the motion vectors texture, we can approximate the movement of snow that used to be at that point by the total distance moved _to_ that pixel from the previous frame. If the speed is high enough, this may by multiple pixels worth of displacement, in which case we need to consider the swept area of the surface over the course of the frame. 

Instead of properly considering the swept area, we can instead approximate by generalize the movement of snow from _all_ points in the motion vectors texture, in those horizontally inside the object. How do we generalize the horizontal surface normal vector even to inside points? If we could generate a [signed distance field](https://en.wikipedia.org/wiki/Signed_distance_function) (SDF) texture from the edge of the object, then by taking the [gradient](https://en.wikipedia.org/wiki/Gradient), we could generate the surface normals, and in fact the [unit vectors pointing towards/away the nearest edge](https://shaderfun.com/2018/07/23/signed-distance-fields-part-8-gradients-bevels-and-noise/). There are many ways to generate the SDF using a shader, such as [brute forcing, the Jump Flooding algorithm](https://shaderbits.com/blog/various-distance-field-generation-techniques), the [dead reckoning algorithm](https://perso.ensta-paris.fr/~manzaner/Download/IAD/Grevera_04.pdf), or others. 

Generating the SDF will be somewhat expensive, so we will instead begin with a simple approximation. First, we will create an image with an [indicator function](https://en.wikipedia.org/wiki/Indicator_function) of the object (1 where the object exists, 0 otherwise), which we can derive easily from the depth bake texture. The negative gradient of the indicator function will give the outward surface normal. This will take nonzero values only at the edges of the indicator function; however, first blurring will increase the thickness where this gradient is nonzero (the result, though, will not be unit vectors). Then, we'll normalize the result so that the vectors at the edges are unit length. To normalize the gradients, we could take the maximum magnitude of the gradient vectors over the texture. Instead, we'll use our knowledge of the blur kernel to determine the maximum possible gradient magnitude, by considering the case of a step function; this will provide an approximate, but inexpensive to calculate, normalizing factor.

Finally, the magnitude of the vector will determine the fraction that we use it to project the motion vector along the surface normal. Therefore, for values far inside the edges, we'll treat the snow motion as being determined entirely by the object motion vectors, while for values at the edges, the snow's motion will be determined by the projection of the object motion vectors onto the surface normal. We'll take a parameterized scaled power of this magnitude to determine the fraction of how much the surface normal is used. Values of these parameters that increase this fraction (low powers, high scales) help to build up snow around the edges of objects.

Note that we could use a more sophisticated approximation to the gradient, such as the [Sobel](https://en.wikipedia.org/wiki/Sobel_operator) or Scharr operator. However, since our indicator function is blurred already, and these filters tend to be convolutions of blurring followed by difference operators, they're less relevant in this case.

#### Motion Vector Dilation

One issue we encounter is that the baked depth is blurred, taking on values outside of the actual px-space positions being rendered to. However, motion vectors will not be rendered to those pixels that take on depth values only due to blurring. We wish to 'spread out' the motion vectors to these neighbouring pixels. 

We can accomplishing this with the [morphological dilation](https://en.wikipedia.org/wiki/Dilation_(morphology)) operator, where we take the gradient vector with the highest magnitude in the structuring element. While not strictly separable, we can approximately perform this in a series of horizontal and vertical dilations (one pixel on each side), for a number of times equal to the blur radius in pixels.

Note that this could lead to potential issues when two objects of different velocities are found horizontally next to each other, and the dilation will cause some motion values of one object to `erode into` the other. By taking the largest magnitude, we at least can ensure that the most important motion is captured. This incorrect dilation of objects into each other is unlikely to be an issue, since in that region the snow will likely simply be compacted due to the depth of those neighboring objects.

#### Update Displacement with Compute Shaders

Recall that our (blurred) per-frame height is blended using `Min` operator to determine the overall mask height (used in detail normal calculation) and, in parallel, also using `Min` operator, the layer mask fraction texture.

Since snow is being displaced, the height will be raised in places, and so we can no longer get away with using only a `Min` operator. Instead, we'll need to add the displacement to the previous height mask, then take the minimum with the current height, to find the new mask height. We'll then need to determine the layer mask CustomRenderTexture directly from that texture.

We can accomplish the last step most straightforwardly, by adding additional passes to the mask update shader to read depth from the mask height texture directly, instead of the baked depth texture.

However, for the rest of what we want to accomplish, we need more than can be accomplished through traditional pixel-by-pixel update, whether through custom passes or CustomRenderTextures. The key difference is that the uv value for the height that needs to be updated due to displacement depends on the result of looking up the motion value at the source uv. We have only forward motion vectors at our disposal, so we cannot look up 'backwards' to determine the source uv. Therefore, we need to be able to access and update the mask height texture in an unknown order, depending on the motion values.

[Unordered access views](https://docs.microsoft.com/en-us/windows/uwp/graphics-concepts/shader-resource-view--srv-) allow use to do just that. Using a [Compute Shader](https://docs.unity3d.com/Manual/class-ComputeShader.html) in Unity will allow us to easily interface with these types of views. In particular, a `RenderTexture` with `enableRandomWrite` set can be used as type `RWTexture2D` that can be accessed to read and write at any indices in the Compute Shader. Note that if we wish to read from as well as write to the same `RWTexture2D` in a compute shader, it needs to be [R32 format](https://docs.microsoft.com/en-us/previous-versions//ff471325(v=vs.85)), so we can use `RFloat` format for this. We could also have used a [`ComputeBuffer`](https://docs.unity3d.com/ScriptReference/ComputeBuffer.html) instead to handle arbitrary data.

We'll dispatch the compute shader for each source pixel in our captured textures (blurred depth, dilation motion, horizontal normals, and mask height). We'll take the difference in height between the previous value (mask height) and the current frame's captured height as the amount of height to undergo displacement. We'll use the motion and horizontal normals to determine the motion amount and direction, as discussed above.

We'll run into issues with this simple approach, particularly at slow speeds, where the snow would always be pushed at most one pixel in front of the moving object. Our blurring of the captured depth buffer will cause the displacement to be immediately lost to the minization. Therefore, we'll need to consider an offset of the motion amount along the motion direction; at minimum, we can take it to be the blur radius in pixels. This will help to form a 'leading edge' in front of moving objects that will give plausible-looking results without simulating the interaction of snow with itself.

After the displacement, we'll dispatch a second compute shader to take the minimum with the current frame's depth, to compact any snow that wasn't displaced away from an object. The edge falloff minimum height will also be considered here, if it exists.

Note that we will use the `CommandBuffer` [`DispatchCompute`](https://docs.unity3d.com/ScriptReference/Rendering.CommandBuffer.DispatchCompute.html) method, instead of the `ComputeShader` [`Dispatch`](https://docs.unity3d.com/ScriptReference/ComputeShader.Dispatch.html) method. In doing so, we schedule the dispatch immediately within our CustomPass, instead of needing to wait for the start of the next frame for the compute shader dispatch to be scheduled automatically.

#### Handling Maximum Height (Initialization)

Previously, we only needed to record the historical lowest occupied height (of the total captured range) at each pixel. From this, the layer mask could be determined by interpolation; crucially, if the top layer's height was less than the lowest occupied height, then the snow was never compacted at this point. Therefore, for unoccupied pixels, the lowest occupied height could be any value above the top layer height, so we were free to initialize this height to the maximum everywhere.

Since we are adding displacement, we face the possibility of piling snow potentially higher than the top layer's height, which cannot be accounted for just using our layer setup. The key takeaway is that any displacement will cause this issue if the snow is initialized with the top layer's full height. We should start the snow at some height _less_ than the maximum, so there is room on top to see the effects of displacement.

Therefore, we'll expose a height offset, multiplier and texture with which to initialize the snow height texture. The offset and multiplier will be with respect to the fractional height range of the top layer; if the texture is not supplied, we'll use the top layer's height texture. We'll need to create a simple `TintedLookup` shader and Material do initialize the texture using `CoreUtils.DrawFullScreen`.

### Improvements to Motion-Vector-Based Displacement

While this method features some _strong_ approximations, it is a straightforward method that pushes snow to the front and to the sides (as dependent on the horizontal shape) of moving objects. It can produce plausible visual results, and is a reasonable starting point for further simulation efforts.

Nevertheless, the consistency of volume of snow being displaced, along with the overall quality of the effect, have room for improvement, which we will explore in this section.

#### Managing Displacement Profile

The contribution of horizontal normal vectors brings the potential for artifacts that wouldn't be seen considering the motion vectors alone. We'll begin by parametrizing the fraction of motion determined by the horizontal normal vector (instead of only the motion vector), by adding in a linear scaling factor, following by a power factor. With this, we can shrink the overall contribution slightly, and with a power greater than 1 focus most of the effect at the horizontal edge of the object. While this works well for our small test spheres, these values can be tweaked depending on the needs.

Additionally, we can see that aliasing issues are responsible for many of these artifacts: the horizontal gradient vector may change rapidly from pixel to pixel, and the amount of displacement from motion will often be on the order of one or a couple pixels. We can take advantage of using a jittering technique, seen in Temoral Anti-Aliasing and other operate on historical results from previous frames. We don't have such a history here, but we can use a subpixel horizontal jitter amount to offset the displaced position of snow in our compute shader. We'll use a low-discrepancy sequence, the [Halton sequence](https://en.wikipedia.org/wiki/Halton_sequence), to set the jitter amount every frame, with a parametrized scaling factor. This helps prevent some of the obvious manifestations of the underlying pixel-based structure of our displacement simulation, particularly when moving in perfectly horizontal or vertical directions.

#### Handling Maximum Height (Simple Spreading)

The snow material has a maximum possible height governed by its topmost layer, parametrized by a value of 1 in the mask height texture that's updated by our displacement kernel. This material cannot physically represent snow higher than this, so we should ensure that our displacement takes this into account. Ideally, we'd like a model of displacement that naturally leads to snow heights less than 1.

Note that the RenderTexture containing the current height values is floating-point format, as chosen to enable read and write in the displacement compute shader kernel. This means that we can track height values larger than 1 in this texture (values higher than the snow material's maximum height range). In contrast, clipping height values to a maximum of 1 would cause displaced snow to be lost forever. By keeping values larger than 1, we can in principle track this snow until it can be displaced or spread out into neighbouring areas than have heights less than 1.

We'll need to update the displacement pass to handle heights larger than one. First, we note that the current occupied height will be 1 in any unoccupied areas, so there will be a nonzero height difference when the mask height is greater than 1, even though there is no object present at those texels. We can check that the current occupied height is less than 1, and ensure that we also remove volume from the source pixel when displacing it, to make sure we only displace when appropriate, and to do so in a volume-neutral way, since we can't only rely on the subsequent minimum pass to ensure that.

One of the most straightforward ways we can prevent snow from piling up at heights greater than 1 is to artifically blur or spread out values larger than one to their neighbours. Our initial implementation will blur a fraction of the amount larger than one to its eight neighbours each frame (weighting less to the diagonals) in another compute shader pass before taking the minimum with the current object height. In practice, this approach has only a small effect, because the blur will often displace that snow into neighbouring pixels that have an object in them, or that are themselves higher than 1, ignoring the physical situation.


#### Simple Velocity Simulation

In the displacement kernel, we used the per-frame, per-pixel horizontal object motion vectors to displace snow height. By subtracting height at the source and addition at the destination, this amount to moving the position of that amount of snow. We could also impart the corresponding _velocity_ to this amount of snow. To do this, we'll need to do the following:

* Impart velocity along with displacement.
* Update snow height each frame due to velocity.
* Update the snow's velocity based on an appropriate model.

Although we add an offset to the motion vectors when determining the displacement (due to our blurring of the depth results), we can simply use the motion vectors as a px/frame velocity. For our data structure, we'll keep a per-px `struct` of horizontal velocity, as well as height, `VelocityData`. Note that this is only the the height amount of snow that's moving, and will generally be smaller than the full snow height at a given pixel. This height will also be parametrized as a fraction of the full snow layer height range. As height is added during the displacement pass, we'll add it to the corresponding velocity-height struct. Since multiple source pixels may add to the same destination pixel, we'll add velocity scaled by height, so that the final velocity can be later retrieved by dividing by the final height of snow in motion.

Updating the snow's height, then, involves making the appropriate subtractions and additions to this data structure. The movement, from frame to frame, of snow in a given pixel, is the frame time multiplied by the velocity of the snow at that source pixel. That movement indicates the target pixel, which will then have added to its `struct` the source's snow height, as well as source velocity scaled by source height (scaled as discussed in the previous paragraph). Since we need to be able to access the previous frame's original velocity, regardless of any additions we have made, we'll need to use a separate buffer for the previous frame's velocity data and the updated one, since the compute kernel would up updating a singular buffer at some pixel ids before those are being read from at other ids.

Finally, we'll need to model the deceleration of the snow. We can do this by computing an updated velocity before applying the previous frame's motion to the snow. We a rough initial implementation, without carefully considering the physics of the situation, we can model the snow's movement with [exponential decay](https://en.wikipedia.org/wiki/Exponential_decay), choosing an appropriate half-life that looks plausible. Since the rate of change is only based in the previous velocity, we only need that, along with the frame time, to determine the velocity at the next frame. In addition to adding decay to the velocity, we'll also add decay to the _amount_ of snow being moved, roughly modelling that some of the snow will 'stick' to the ground. We'll also add the reverse, where some existing snow gets 'picked up' by the moving snow, depending on how high the existing snow is relative to the amount being moved.

#### Accessing Shared Memory with Atomic Functions

Both displacement and the velocity simulation involve updating height or velocity values of a buffer at a texel location that is dependent on looking up motion data. We've already seen the need to use [double buffering](https://en.wikipedia.org/wiki/Multiple_buffering) to ensure that the previous frame's data can always be accessed, even if some texels of the result have already been updated. However, there is still an issue, in that multiple source texels may update data at the same target texel, _potentially at the same time_, thanks to multiple thread groups. 

This is exacerbated by the fact that the target texel will almost always be a fractional texel ID, so to avoid rounding issues, the additional height or velocity must be proportionally allocated to the four closest texels. The chance of overwrites, then, is significant.

We'll need to use [atomic operations](https://docs.microsoft.com/en-us/windows/win32/direct3d11/direct3d-11-advanced-stages-cs-atomic-functions) in order to ensure we don't run into timing issues. Mostly, we'll need the [`InterlockedAdd`](https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/interlockedadd) function, so we can update height and velocity through addition, without worrying about other evaluations potentially changing the target value.

However, support for atomic add operations is [limited to](https://docs.microsoft.com/en-us/previous-versions//ff471325(v=vs.85)#uav-atomic-add) 32-bit integer or unsigned integers, not the 32-bit floating-point values we'd be using previously. These formats are the only other ones (beyond 32-bit floating-point) supported by `RWTexture2D`, or similarly, `RWBuffer` and `RWStructuredBuffer`. As a result, we'll need to encode our floating-point values as integers in our data structues to use `InterlockedAdd`.

We can immediately see a significant reduction in artifacts, particularly from the velocity simulation, by switching to these atomic functions.

When we need to sample the previous frame's values, for either height or velocity, we can sample from decoded floating-point data, a `StructuredBuffer<VelocityData>` for velocity, and `Texture2D<float>` for height. We can sample from the `Buffer<int>` of encoded height values in a pixel shader to perform the height decoding into the floating-point mask height texture.

#### Other Simple Simulation Improvements

Instead of exponential decay, we'll choose the deceleration to be based on some power of the velocity, other than 1, so that we have more control over the motion, since we generally want stronger decrease at higher velocities. For now, we'll simply eyeball parameters that look reasonable, without too strong a consideration for the underlying physical model.

By considering the height difference when moving snow in the velocity simulation kernel, we can begin to model the difficulty in pushing snow uphill. In this case, we'll limit the amount of snow being moved based on the increase in height from the snow's previous position. Amounts of snow being moved below this height difference will be impeded, but the amounts above can move freely. Therefore, we'll model the amount moving up to the height difference as only partially being able to move, which we'll again model using exponential decay of this fraction of the height amount being moved.

By considering the amount of height above 1 (the maximum representable height) in the velocity simulation kernel, we can move excess snow in a more physically-meaningful manner than the simple spread kernel. We can simply increase the amount of height moved to include any amount greater than 1, if needed. We will also reduce the amount of velocity reduction the more height we had to include in this way, in order to help this 'excess height' travel at meaningful speeds to areas where snow height is less than 1, and it can be deposited in a representable way.

With this, we may not need to call the height spread kernel at all, so we'll parameterize that choice, as well. However, we'll increase the flexibility of the spread kernel by parameterizing the amount of spread with a half-life, and also introduce a similarly-parameterized decay amount, which will simply reduce any height above one.

In many cases, particularly with larger or more gently-sloped objects, displacement of snow height will cause the snow to be moved to a texel where an object exists, and snow has similarly been moved from there in order to match the currently-occupied height. This means that snow moved *to* that texel will end up being compacted by the minimum pass, due to the presence of the object. These issues are not unexpected given the simple model of snow displacement, which doesn't model the horizontal forces of the snow, or how snow movement affects horizontally-neighbouring snow. However, by simply _repeating_ the displacement kernel some number of additional times, snow that might otherwise be lost in the above manner can end up being pushed to the front of a moving object. Given the performance hit for repeating the this kernel, and that the shape and size of the moving object affects the number of repetitions needed to fully move all snow in this manner, we won't try to automatically determine the appropriate number of repetitions. Instead, we'll parameterize it, and leave it up to the user's discretion.

#### Handling Swept Area

When small objects move a large distance from one frame to the next, simply updating the snow depth at each frame means that the area in-between is unaffected. This can lead to 'holes' in the snow at the locations where the object happened to be when frames were captured, with snow in between remaining at full height. 

However, we may approximate the movement at each pixel as linear motion, based on the captured motion vectors. From the motion vectors, we can trace a line from each pixel back to its position in the previous frame. This gives us the path travelled by that portion of the object (with its corresponding height) from the previous frame. By considering each pixel on this line using [line rasterization](https://en.wikipedia.org/wiki/Line_drawing_algorithm), we can again apply the height minimization based on the final pixel's current object height. Additionally, if any of those pixels have no motion vectors, we can apply to them the motion vectors for the final pixel.

We can update the captured height and motion vector textures directly in a compute kernel by ensure that their source `RTHandle` have `enableRandomWrite` set. Note that multiple of these lines may overlap the same pixel within the swept area, and due to multiple thread groups, are in principle writing in a non-thread-safe manner. We could use atomic operations such as [`InterlockedMin`](https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/interlockedmin), but those only work on integer-type resources. However, since we don't see the kind of artifacts from this that we did with the displacement and velocity simulation kernels, we opt to leave the implementation as-is for now.


### Future Motion-Vector Displacement Work

* Combine displacement and height-above-one spread compute kernels, to save decoding height between executing those kernels.
* Perform encoding and decoding of ComputeBuffers in pixel shaders, if more efficient than doing so using Compute shaders.
* Improved modelling of dynamics, including acceleration/deceleration downhill and uphill.
* Consider improvements to the combination of horizontal normal vectors and motion vectors in the displacement pass, as needed.
* Consider improved handling of dynamics and spreading of height above the maximum representable value, repspecting neighbouring object depths, as needed.
* Use atomic operations in swept area kernel, along with the appropriate buffers.

## Complex Simulation

### Future Complex Simulation Work

* Investigate implementation of full snow simulation.
* Investigate rendering representations of full snow simulation.
* Implementation of the above.

## Other Future Work

* General code and architecture improvement and cleanup.